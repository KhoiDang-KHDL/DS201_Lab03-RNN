{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Git","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/KhoiDang-KHDL/DS201_Lab03-RNN\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:33.073623Z","iopub.execute_input":"2025-11-26T08:07:33.073911Z","iopub.status.idle":"2025-11-26T08:07:34.333991Z","shell.execute_reply.started":"2025-11-26T08:07:33.073887Z","shell.execute_reply":"2025-11-26T08:07:34.333089Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'DS201_Lab03-RNN'...\nremote: Enumerating objects: 43, done.\u001b[K\nremote: Counting objects: 100% (43/43), done.\u001b[K\nremote: Compressing objects: 100% (33/33), done.\u001b[K\nremote: Total 43 (delta 21), reused 25 (delta 8), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (43/43), 1.10 MiB | 8.69 MiB/s, done.\nResolving deltas: 100% (21/21), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/DS201_Lab03-RNN') \n\n# Check thư mục\nimport os\nprint(os.listdir('/kaggle/working/DS201_Lab03-RNN'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:34.335145Z","iopub.execute_input":"2025-11-26T08:07:34.335367Z","iopub.status.idle":"2025-11-26T08:07:34.341236Z","shell.execute_reply.started":"2025-11-26T08:07:34.335343Z","shell.execute_reply":"2025-11-26T08:07:34.340452Z"}},"outputs":[{"name":"stdout","text":"['UIT-VSFC-20251121T013658Z-1-001', 'lstm.py', 'uit_vsfc.py', 'PhoNer.py', 'seq2seq.py', 'gru.py', 'PhoNER', '.git']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Thư viện","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport os\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\nimport uit_vsfc\n\nfrom uit_vsfc import Vocab, UIT_VSFC, collate_fn\nfrom lstm import LSTMModel\nfrom gru import GRUModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:34.343201Z","iopub.execute_input":"2025-11-26T08:07:34.343599Z","iopub.status.idle":"2025-11-26T08:07:41.411995Z","shell.execute_reply.started":"2025-11-26T08:07:34.343578Z","shell.execute_reply":"2025-11-26T08:07:41.411206Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Cấu hình cho bài 1,2","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nHIDDEN_SIZE = 128  \nN_LAYERS = 2\nLEARNING_RATE = 0.001\nEPOCHS = 10\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:41.412760Z","iopub.execute_input":"2025-11-26T08:07:41.413048Z","iopub.status.idle":"2025-11-26T08:07:41.501728Z","shell.execute_reply.started":"2025-11-26T08:07:41.413030Z","shell.execute_reply":"2025-11-26T08:07:41.500735Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Train, Eval bài 1,2","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n            \n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            # Lấy nhãn dự đoán\n            _, predicted = torch.max(outputs, 1)\n            \n            # Chuyển về cpu để tính trong sklearn\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    avg_loss = total_loss / len(dataloader)\n    \n    # Tính Accuracy và F1-Score\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    return avg_loss, acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:41.502867Z","iopub.execute_input":"2025-11-26T08:07:41.503189Z","iopub.status.idle":"2025-11-26T08:07:41.613440Z","shell.execute_reply.started":"2025-11-26T08:07:41.503160Z","shell.execute_reply":"2025-11-26T08:07:41.612875Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train(model, train_loader, dev_loader, optimizer, criterion, epochs):\n    print(f\"Bắt đầu huấn luyện trên thiết bị: {DEVICE}\")\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        \n        for i, batch in enumerate(train_loader):\n            input_ids = batch['input_ids'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n        dev_loss, dev_acc, dev_f1 = evaluate(model, dev_loader, criterion)\n        print(f\"--- Hết Epoch {epoch+1} ---\")\n        print(f\"Train Loss: {running_loss/len(train_loader):.4f}\")\n        print(f\"Dev Loss: {dev_loss:.4f} | Dev Acc: {dev_acc*100:.2f}% | Dev F1: {dev_f1*100:.2f}%\")\n        print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:41.614205Z","iopub.execute_input":"2025-11-26T08:07:41.614456Z","iopub.status.idle":"2025-11-26T08:07:41.632374Z","shell.execute_reply.started":"2025-11-26T08:07:41.614438Z","shell.execute_reply":"2025-11-26T08:07:41.631702Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Bài 1: main","metadata":{}},{"cell_type":"code","source":"def main():\n\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/UIT-VSFC-20251121T013658Z-1-001/UIT-VSFC'\n    \n    train_path = os.path.join(base_path, 'UIT-VSFC-train.json')\n    dev_path = os.path.join(base_path, 'UIT-VSFC-dev.json')\n    test_path = os.path.join(base_path, 'UIT-VSFC-test.json')\n\n    print(\"Đang xây dựng bộ từ điển (Vocab):\")\n    vocab = Vocab(base_path)\n    print(f\"Kích thước từ điển: {vocab.len}\")\n\n    print(\"Đang tải dữ liệu...\")\n    train_dataset = UIT_VSFC(train_path, vocab)\n    dev_dataset = UIT_VSFC(dev_path, vocab)\n    test_dataset = UIT_VSFC(test_path, vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # --- MODEL ---\n    model = LSTMModel(\n        vocab_size=vocab.len,\n        hidden_size=HIDDEN_SIZE,\n        n_layers=N_LAYERS,\n        n_labels=vocab.n_labels,\n        padding_idx=vocab.w2i[vocab.pad]\n    ).to(DEVICE)\n\n    # ---  OPTIMIZER, LOSS ---\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train(model, train_loader, dev_loader, optimizer, criterion, EPOCHS)\n\n    # Evaluate\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"Test F1-Score: {test_f1*100:.2f}%\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:41.633019Z","iopub.execute_input":"2025-11-26T08:07:41.633191Z","iopub.status.idle":"2025-11-26T08:07:58.234399Z","shell.execute_reply.started":"2025-11-26T08:07:41.633178Z","shell.execute_reply":"2025-11-26T08:07:58.233709Z"}},"outputs":[{"name":"stdout","text":"Đang xây dựng bộ từ điển (Vocab):\nKích thước từ điển: 2848\nĐang tải dữ liệu...\nBắt đầu huấn luyện trên thiết bị: cuda\nEpoch [1/10], Step [10/179], Loss: 1.0089\nEpoch [1/10], Step [20/179], Loss: 0.7847\nEpoch [1/10], Step [30/179], Loss: 0.7138\nEpoch [1/10], Step [40/179], Loss: 0.7400\nEpoch [1/10], Step [50/179], Loss: 0.6469\nEpoch [1/10], Step [60/179], Loss: 0.8894\nEpoch [1/10], Step [70/179], Loss: 0.8819\nEpoch [1/10], Step [80/179], Loss: 0.8493\nEpoch [1/10], Step [90/179], Loss: 0.9663\nEpoch [1/10], Step [100/179], Loss: 0.7485\nEpoch [1/10], Step [110/179], Loss: 0.7659\nEpoch [1/10], Step [120/179], Loss: 0.7559\nEpoch [1/10], Step [130/179], Loss: 1.0587\nEpoch [1/10], Step [140/179], Loss: 0.8138\nEpoch [1/10], Step [150/179], Loss: 0.6716\nEpoch [1/10], Step [160/179], Loss: 0.7713\nEpoch [1/10], Step [170/179], Loss: 0.7498\n--- Hết Epoch 1 ---\nTrain Loss: 0.8699\nDev Loss: 0.8391 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [2/10], Step [10/179], Loss: 0.8451\nEpoch [2/10], Step [20/179], Loss: 0.8997\nEpoch [2/10], Step [30/179], Loss: 0.8561\nEpoch [2/10], Step [40/179], Loss: 0.7788\nEpoch [2/10], Step [50/179], Loss: 0.9659\nEpoch [2/10], Step [60/179], Loss: 0.7898\nEpoch [2/10], Step [70/179], Loss: 0.7959\nEpoch [2/10], Step [80/179], Loss: 0.7816\nEpoch [2/10], Step [90/179], Loss: 0.7692\nEpoch [2/10], Step [100/179], Loss: 0.7359\nEpoch [2/10], Step [110/179], Loss: 0.8502\nEpoch [2/10], Step [120/179], Loss: 0.8806\nEpoch [2/10], Step [130/179], Loss: 0.7157\nEpoch [2/10], Step [140/179], Loss: 0.7984\nEpoch [2/10], Step [150/179], Loss: 0.8034\nEpoch [2/10], Step [160/179], Loss: 0.8353\nEpoch [2/10], Step [170/179], Loss: 0.9720\n--- Hết Epoch 2 ---\nTrain Loss: 0.8313\nDev Loss: 0.7996 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [3/10], Step [10/179], Loss: 0.8482\nEpoch [3/10], Step [20/179], Loss: 0.8597\nEpoch [3/10], Step [30/179], Loss: 0.8489\nEpoch [3/10], Step [40/179], Loss: 0.8572\nEpoch [3/10], Step [50/179], Loss: 0.7477\nEpoch [3/10], Step [60/179], Loss: 0.8664\nEpoch [3/10], Step [70/179], Loss: 0.9700\nEpoch [3/10], Step [80/179], Loss: 0.6725\nEpoch [3/10], Step [90/179], Loss: 0.5497\nEpoch [3/10], Step [100/179], Loss: 0.8958\nEpoch [3/10], Step [110/179], Loss: 0.8133\nEpoch [3/10], Step [120/179], Loss: 0.7539\nEpoch [3/10], Step [130/179], Loss: 0.9602\nEpoch [3/10], Step [140/179], Loss: 0.9950\nEpoch [3/10], Step [150/179], Loss: 0.7398\nEpoch [3/10], Step [160/179], Loss: 0.9627\nEpoch [3/10], Step [170/179], Loss: 0.8159\n--- Hết Epoch 3 ---\nTrain Loss: 0.8408\nDev Loss: 0.8442 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [4/10], Step [10/179], Loss: 0.8006\nEpoch [4/10], Step [20/179], Loss: 0.9070\nEpoch [4/10], Step [30/179], Loss: 0.6772\nEpoch [4/10], Step [40/179], Loss: 0.9233\nEpoch [4/10], Step [50/179], Loss: 0.9197\nEpoch [4/10], Step [60/179], Loss: 1.0325\nEpoch [4/10], Step [70/179], Loss: 0.8210\nEpoch [4/10], Step [80/179], Loss: 0.8335\nEpoch [4/10], Step [90/179], Loss: 0.8773\nEpoch [4/10], Step [100/179], Loss: 0.8388\nEpoch [4/10], Step [110/179], Loss: 0.8771\nEpoch [4/10], Step [120/179], Loss: 0.9146\nEpoch [4/10], Step [130/179], Loss: 0.9664\nEpoch [4/10], Step [140/179], Loss: 0.7041\nEpoch [4/10], Step [150/179], Loss: 0.7897\nEpoch [4/10], Step [160/179], Loss: 0.8987\nEpoch [4/10], Step [170/179], Loss: 0.7551\n--- Hết Epoch 4 ---\nTrain Loss: 0.8421\nDev Loss: 0.8391 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [5/10], Step [10/179], Loss: 0.7778\nEpoch [5/10], Step [20/179], Loss: 0.7371\nEpoch [5/10], Step [30/179], Loss: 0.4965\nEpoch [5/10], Step [40/179], Loss: 0.9499\nEpoch [5/10], Step [50/179], Loss: 0.9032\nEpoch [5/10], Step [60/179], Loss: 0.9464\nEpoch [5/10], Step [70/179], Loss: 0.7721\nEpoch [5/10], Step [80/179], Loss: 0.7035\nEpoch [5/10], Step [90/179], Loss: 0.9603\nEpoch [5/10], Step [100/179], Loss: 0.8636\nEpoch [5/10], Step [110/179], Loss: 0.7961\nEpoch [5/10], Step [120/179], Loss: 0.8609\nEpoch [5/10], Step [130/179], Loss: 0.7803\nEpoch [5/10], Step [140/179], Loss: 0.8085\nEpoch [5/10], Step [150/179], Loss: 0.8142\nEpoch [5/10], Step [160/179], Loss: 0.7764\nEpoch [5/10], Step [170/179], Loss: 0.8953\n--- Hết Epoch 5 ---\nTrain Loss: 0.8437\nDev Loss: 0.8472 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [6/10], Step [10/179], Loss: 0.8811\nEpoch [6/10], Step [20/179], Loss: 0.9282\nEpoch [6/10], Step [30/179], Loss: 0.8626\nEpoch [6/10], Step [40/179], Loss: 0.7962\nEpoch [6/10], Step [50/179], Loss: 0.8539\nEpoch [6/10], Step [60/179], Loss: 0.8581\nEpoch [6/10], Step [70/179], Loss: 0.9484\nEpoch [6/10], Step [80/179], Loss: 0.8974\nEpoch [6/10], Step [90/179], Loss: 0.7126\nEpoch [6/10], Step [100/179], Loss: 0.8301\nEpoch [6/10], Step [110/179], Loss: 0.7394\nEpoch [6/10], Step [120/179], Loss: 0.9283\nEpoch [6/10], Step [130/179], Loss: 0.8775\nEpoch [6/10], Step [140/179], Loss: 0.7823\nEpoch [6/10], Step [150/179], Loss: 0.9537\nEpoch [6/10], Step [160/179], Loss: 0.9023\nEpoch [6/10], Step [170/179], Loss: 0.9150\n--- Hết Epoch 6 ---\nTrain Loss: 0.8425\nDev Loss: 0.8415 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [7/10], Step [10/179], Loss: 0.9712\nEpoch [7/10], Step [20/179], Loss: 0.8904\nEpoch [7/10], Step [30/179], Loss: 0.8854\nEpoch [7/10], Step [40/179], Loss: 0.6246\nEpoch [7/10], Step [50/179], Loss: 0.9667\nEpoch [7/10], Step [60/179], Loss: 0.7915\nEpoch [7/10], Step [70/179], Loss: 0.9676\nEpoch [7/10], Step [80/179], Loss: 0.7728\nEpoch [7/10], Step [90/179], Loss: 0.8844\nEpoch [7/10], Step [100/179], Loss: 0.9072\nEpoch [7/10], Step [110/179], Loss: 0.8177\nEpoch [7/10], Step [120/179], Loss: 0.9153\nEpoch [7/10], Step [130/179], Loss: 0.7356\nEpoch [7/10], Step [140/179], Loss: 0.7422\nEpoch [7/10], Step [150/179], Loss: 0.8225\nEpoch [7/10], Step [160/179], Loss: 0.7524\nEpoch [7/10], Step [170/179], Loss: 0.8078\n--- Hết Epoch 7 ---\nTrain Loss: 0.8428\nDev Loss: 0.8402 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [8/10], Step [10/179], Loss: 0.8595\nEpoch [8/10], Step [20/179], Loss: 1.0670\nEpoch [8/10], Step [30/179], Loss: 0.7461\nEpoch [8/10], Step [40/179], Loss: 0.7393\nEpoch [8/10], Step [50/179], Loss: 0.8599\nEpoch [8/10], Step [60/179], Loss: 0.7716\nEpoch [8/10], Step [70/179], Loss: 0.9030\nEpoch [8/10], Step [80/179], Loss: 0.7703\nEpoch [8/10], Step [90/179], Loss: 0.8145\nEpoch [8/10], Step [100/179], Loss: 0.7115\nEpoch [8/10], Step [110/179], Loss: 0.8120\nEpoch [8/10], Step [120/179], Loss: 0.8795\nEpoch [8/10], Step [130/179], Loss: 0.8201\nEpoch [8/10], Step [140/179], Loss: 0.7993\nEpoch [8/10], Step [150/179], Loss: 0.6980\nEpoch [8/10], Step [160/179], Loss: 0.8391\nEpoch [8/10], Step [170/179], Loss: 0.7805\n--- Hết Epoch 8 ---\nTrain Loss: 0.8429\nDev Loss: 0.8408 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [9/10], Step [10/179], Loss: 0.6581\nEpoch [9/10], Step [20/179], Loss: 0.8237\nEpoch [9/10], Step [30/179], Loss: 0.9430\nEpoch [9/10], Step [40/179], Loss: 0.8196\nEpoch [9/10], Step [50/179], Loss: 0.8598\nEpoch [9/10], Step [60/179], Loss: 0.7964\nEpoch [9/10], Step [70/179], Loss: 0.7530\nEpoch [9/10], Step [80/179], Loss: 0.9585\nEpoch [9/10], Step [90/179], Loss: 0.6924\nEpoch [9/10], Step [100/179], Loss: 0.7282\nEpoch [9/10], Step [110/179], Loss: 0.7461\nEpoch [9/10], Step [120/179], Loss: 0.9759\nEpoch [9/10], Step [130/179], Loss: 0.8297\nEpoch [9/10], Step [140/179], Loss: 0.8196\nEpoch [9/10], Step [150/179], Loss: 0.7946\nEpoch [9/10], Step [160/179], Loss: 0.9435\nEpoch [9/10], Step [170/179], Loss: 0.9989\n--- Hết Epoch 9 ---\nTrain Loss: 0.8419\nDev Loss: 0.8438 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\nEpoch [10/10], Step [10/179], Loss: 0.8180\nEpoch [10/10], Step [20/179], Loss: 1.1843\nEpoch [10/10], Step [30/179], Loss: 0.9232\nEpoch [10/10], Step [40/179], Loss: 1.0372\nEpoch [10/10], Step [50/179], Loss: 0.8368\nEpoch [10/10], Step [60/179], Loss: 1.1087\nEpoch [10/10], Step [70/179], Loss: 0.6409\nEpoch [10/10], Step [80/179], Loss: 0.9822\nEpoch [10/10], Step [90/179], Loss: 1.0276\nEpoch [10/10], Step [100/179], Loss: 0.6942\nEpoch [10/10], Step [110/179], Loss: 0.6914\nEpoch [10/10], Step [120/179], Loss: 0.5981\nEpoch [10/10], Step [130/179], Loss: 0.7111\nEpoch [10/10], Step [140/179], Loss: 0.8154\nEpoch [10/10], Step [150/179], Loss: 0.7794\nEpoch [10/10], Step [160/179], Loss: 0.8154\nEpoch [10/10], Step [170/179], Loss: 0.9001\n--- Hết Epoch 10 ---\nTrain Loss: 0.8431\nDev Loss: 0.8425 | Dev Acc: 72.71% | Dev F1: 21.05%\n------------------------------\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.8351\nTest Accuracy: 72.33%\nTest F1-Score: 20.99%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Bài 2: main","metadata":{}},{"cell_type":"code","source":"def main():\n\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/UIT-VSFC-20251121T013658Z-1-001/UIT-VSFC'\n    \n    train_path = os.path.join(base_path, 'UIT-VSFC-train.json')\n    dev_path = os.path.join(base_path, 'UIT-VSFC-dev.json')\n    test_path = os.path.join(base_path, 'UIT-VSFC-test.json')\n\n    print(\"Đang xây dựng bộ từ điển (Vocab)...\")\n    vocab = Vocab(base_path)\n    print(f\"Kích thước từ điển: {vocab.len}\")\n\n    print(\"Đang tải dữ liệu...\")\n    train_dataset = UIT_VSFC(train_path, vocab)\n    dev_dataset = UIT_VSFC(dev_path, vocab)\n    test_dataset = UIT_VSFC(test_path, vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # ---  MODEL ---\n    model = GRUModel(\n        vocab_size=vocab.len,\n        embedding_dim=128,\n        hidden_size=HIDDEN_SIZE,\n        n_layers=N_LAYERS,\n        n_labels=vocab.n_labels,\n        padding_idx=vocab.w2i[vocab.pad]\n    ).to(DEVICE)\n\n    # ---  OPTIMIZER, LOSS ---\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train(model, train_loader, dev_loader, optimizer, criterion, EPOCHS)\n    \n    # Evaluate\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"Test F1-Score: {test_f1*100:.2f}%\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:07:58.235230Z","iopub.execute_input":"2025-11-26T08:07:58.235642Z","iopub.status.idle":"2025-11-26T08:08:12.936449Z","shell.execute_reply.started":"2025-11-26T08:07:58.235610Z","shell.execute_reply":"2025-11-26T08:08:12.935861Z"}},"outputs":[{"name":"stdout","text":"Đang xây dựng bộ từ điển (Vocab)...\nKích thước từ điển: 2848\nĐang tải dữ liệu...\nBắt đầu huấn luyện trên thiết bị: cuda\nEpoch [1/10], Step [10/179], Loss: 0.6835\nEpoch [1/10], Step [20/179], Loss: 0.6225\nEpoch [1/10], Step [30/179], Loss: 0.6225\nEpoch [1/10], Step [40/179], Loss: 0.8219\nEpoch [1/10], Step [50/179], Loss: 0.5715\nEpoch [1/10], Step [60/179], Loss: 0.5075\nEpoch [1/10], Step [70/179], Loss: 0.2805\nEpoch [1/10], Step [80/179], Loss: 0.6149\nEpoch [1/10], Step [90/179], Loss: 0.4467\nEpoch [1/10], Step [100/179], Loss: 0.3397\nEpoch [1/10], Step [110/179], Loss: 0.3065\nEpoch [1/10], Step [120/179], Loss: 0.4243\nEpoch [1/10], Step [130/179], Loss: 0.4245\nEpoch [1/10], Step [140/179], Loss: 0.3513\nEpoch [1/10], Step [150/179], Loss: 0.3810\nEpoch [1/10], Step [160/179], Loss: 0.3836\nEpoch [1/10], Step [170/179], Loss: 0.3128\n--- Hết Epoch 1 ---\nTrain Loss: 0.4699\nDev Loss: 0.3736 | Dev Acc: 86.54% | Dev F1: 71.99%\n------------------------------\nEpoch [2/10], Step [10/179], Loss: 0.4038\nEpoch [2/10], Step [20/179], Loss: 0.2160\nEpoch [2/10], Step [30/179], Loss: 0.2478\nEpoch [2/10], Step [40/179], Loss: 0.3713\nEpoch [2/10], Step [50/179], Loss: 0.3321\nEpoch [2/10], Step [60/179], Loss: 0.3807\nEpoch [2/10], Step [70/179], Loss: 0.3482\nEpoch [2/10], Step [80/179], Loss: 0.3759\nEpoch [2/10], Step [90/179], Loss: 0.3567\nEpoch [2/10], Step [100/179], Loss: 0.2711\nEpoch [2/10], Step [110/179], Loss: 0.6489\nEpoch [2/10], Step [120/179], Loss: 0.4086\nEpoch [2/10], Step [130/179], Loss: 0.3121\nEpoch [2/10], Step [140/179], Loss: 0.2774\nEpoch [2/10], Step [150/179], Loss: 0.4307\nEpoch [2/10], Step [160/179], Loss: 0.4033\nEpoch [2/10], Step [170/179], Loss: 0.2760\n--- Hết Epoch 2 ---\nTrain Loss: 0.3221\nDev Loss: 0.3627 | Dev Acc: 86.29% | Dev F1: 74.99%\n------------------------------\nEpoch [3/10], Step [10/179], Loss: 0.3180\nEpoch [3/10], Step [20/179], Loss: 0.2535\nEpoch [3/10], Step [30/179], Loss: 0.2223\nEpoch [3/10], Step [40/179], Loss: 0.3197\nEpoch [3/10], Step [50/179], Loss: 0.2955\nEpoch [3/10], Step [60/179], Loss: 0.2005\nEpoch [3/10], Step [70/179], Loss: 0.2432\nEpoch [3/10], Step [80/179], Loss: 0.2206\nEpoch [3/10], Step [90/179], Loss: 0.3351\nEpoch [3/10], Step [100/179], Loss: 0.2759\nEpoch [3/10], Step [110/179], Loss: 0.2329\nEpoch [3/10], Step [120/179], Loss: 0.1987\nEpoch [3/10], Step [130/179], Loss: 0.3545\nEpoch [3/10], Step [140/179], Loss: 0.2147\nEpoch [3/10], Step [150/179], Loss: 0.2190\nEpoch [3/10], Step [160/179], Loss: 0.3985\nEpoch [3/10], Step [170/179], Loss: 0.2560\n--- Hết Epoch 3 ---\nTrain Loss: 0.2744\nDev Loss: 0.3504 | Dev Acc: 87.62% | Dev F1: 75.95%\n------------------------------\nEpoch [4/10], Step [10/179], Loss: 0.1369\nEpoch [4/10], Step [20/179], Loss: 0.2122\nEpoch [4/10], Step [30/179], Loss: 0.3227\nEpoch [4/10], Step [40/179], Loss: 0.1595\nEpoch [4/10], Step [50/179], Loss: 0.1029\nEpoch [4/10], Step [60/179], Loss: 0.2323\nEpoch [4/10], Step [70/179], Loss: 0.2869\nEpoch [4/10], Step [80/179], Loss: 0.1404\nEpoch [4/10], Step [90/179], Loss: 0.2100\nEpoch [4/10], Step [100/179], Loss: 0.3515\nEpoch [4/10], Step [110/179], Loss: 0.1830\nEpoch [4/10], Step [120/179], Loss: 0.2345\nEpoch [4/10], Step [130/179], Loss: 0.1033\nEpoch [4/10], Step [140/179], Loss: 0.1217\nEpoch [4/10], Step [150/179], Loss: 0.2610\nEpoch [4/10], Step [160/179], Loss: 0.1993\nEpoch [4/10], Step [170/179], Loss: 0.1808\n--- Hết Epoch 4 ---\nTrain Loss: 0.2279\nDev Loss: 0.3921 | Dev Acc: 86.99% | Dev F1: 74.30%\n------------------------------\nEpoch [5/10], Step [10/179], Loss: 0.0529\nEpoch [5/10], Step [20/179], Loss: 0.2310\nEpoch [5/10], Step [30/179], Loss: 0.0622\nEpoch [5/10], Step [40/179], Loss: 0.1862\nEpoch [5/10], Step [50/179], Loss: 0.1768\nEpoch [5/10], Step [60/179], Loss: 0.1058\nEpoch [5/10], Step [70/179], Loss: 0.1214\nEpoch [5/10], Step [80/179], Loss: 0.1458\nEpoch [5/10], Step [90/179], Loss: 0.2605\nEpoch [5/10], Step [100/179], Loss: 0.1117\nEpoch [5/10], Step [110/179], Loss: 0.1721\nEpoch [5/10], Step [120/179], Loss: 0.2063\nEpoch [5/10], Step [130/179], Loss: 0.3209\nEpoch [5/10], Step [140/179], Loss: 0.0810\nEpoch [5/10], Step [150/179], Loss: 0.1823\nEpoch [5/10], Step [160/179], Loss: 0.2130\nEpoch [5/10], Step [170/179], Loss: 0.2151\n--- Hết Epoch 5 ---\nTrain Loss: 0.1857\nDev Loss: 0.3909 | Dev Acc: 87.18% | Dev F1: 76.91%\n------------------------------\nEpoch [6/10], Step [10/179], Loss: 0.0544\nEpoch [6/10], Step [20/179], Loss: 0.0856\nEpoch [6/10], Step [30/179], Loss: 0.0263\nEpoch [6/10], Step [40/179], Loss: 0.0363\nEpoch [6/10], Step [50/179], Loss: 0.1462\nEpoch [6/10], Step [60/179], Loss: 0.0995\nEpoch [6/10], Step [70/179], Loss: 0.1380\nEpoch [6/10], Step [80/179], Loss: 0.1088\nEpoch [6/10], Step [90/179], Loss: 0.1172\nEpoch [6/10], Step [100/179], Loss: 0.4683\nEpoch [6/10], Step [110/179], Loss: 0.0867\nEpoch [6/10], Step [120/179], Loss: 0.2584\nEpoch [6/10], Step [130/179], Loss: 0.1829\nEpoch [6/10], Step [140/179], Loss: 0.1800\nEpoch [6/10], Step [150/179], Loss: 0.2553\nEpoch [6/10], Step [160/179], Loss: 0.1490\nEpoch [6/10], Step [170/179], Loss: 0.0853\n--- Hết Epoch 6 ---\nTrain Loss: 0.1490\nDev Loss: 0.4263 | Dev Acc: 86.67% | Dev F1: 75.54%\n------------------------------\nEpoch [7/10], Step [10/179], Loss: 0.0742\nEpoch [7/10], Step [20/179], Loss: 0.1827\nEpoch [7/10], Step [30/179], Loss: 0.1853\nEpoch [7/10], Step [40/179], Loss: 0.1400\nEpoch [7/10], Step [50/179], Loss: 0.1318\nEpoch [7/10], Step [60/179], Loss: 0.0337\nEpoch [7/10], Step [70/179], Loss: 0.0741\nEpoch [7/10], Step [80/179], Loss: 0.1045\nEpoch [7/10], Step [90/179], Loss: 0.1249\nEpoch [7/10], Step [100/179], Loss: 0.1237\nEpoch [7/10], Step [110/179], Loss: 0.0721\nEpoch [7/10], Step [120/179], Loss: 0.0732\nEpoch [7/10], Step [130/179], Loss: 0.1625\nEpoch [7/10], Step [140/179], Loss: 0.1114\nEpoch [7/10], Step [150/179], Loss: 0.1269\nEpoch [7/10], Step [160/179], Loss: 0.2423\nEpoch [7/10], Step [170/179], Loss: 0.1805\n--- Hết Epoch 7 ---\nTrain Loss: 0.1087\nDev Loss: 0.5113 | Dev Acc: 86.48% | Dev F1: 74.91%\n------------------------------\nEpoch [8/10], Step [10/179], Loss: 0.1183\nEpoch [8/10], Step [20/179], Loss: 0.0400\nEpoch [8/10], Step [30/179], Loss: 0.0514\nEpoch [8/10], Step [40/179], Loss: 0.0188\nEpoch [8/10], Step [50/179], Loss: 0.1866\nEpoch [8/10], Step [60/179], Loss: 0.0855\nEpoch [8/10], Step [70/179], Loss: 0.0446\nEpoch [8/10], Step [80/179], Loss: 0.0282\nEpoch [8/10], Step [90/179], Loss: 0.1292\nEpoch [8/10], Step [100/179], Loss: 0.0700\nEpoch [8/10], Step [110/179], Loss: 0.0936\nEpoch [8/10], Step [120/179], Loss: 0.1598\nEpoch [8/10], Step [130/179], Loss: 0.1352\nEpoch [8/10], Step [140/179], Loss: 0.1764\nEpoch [8/10], Step [150/179], Loss: 0.0590\nEpoch [8/10], Step [160/179], Loss: 0.0951\nEpoch [8/10], Step [170/179], Loss: 0.0565\n--- Hết Epoch 8 ---\nTrain Loss: 0.0896\nDev Loss: 0.5207 | Dev Acc: 87.11% | Dev F1: 75.79%\n------------------------------\nEpoch [9/10], Step [10/179], Loss: 0.0789\nEpoch [9/10], Step [20/179], Loss: 0.0217\nEpoch [9/10], Step [30/179], Loss: 0.0311\nEpoch [9/10], Step [40/179], Loss: 0.0316\nEpoch [9/10], Step [50/179], Loss: 0.1152\nEpoch [9/10], Step [60/179], Loss: 0.0087\nEpoch [9/10], Step [70/179], Loss: 0.0429\nEpoch [9/10], Step [80/179], Loss: 0.1653\nEpoch [9/10], Step [90/179], Loss: 0.0886\nEpoch [9/10], Step [100/179], Loss: 0.0851\nEpoch [9/10], Step [110/179], Loss: 0.1401\nEpoch [9/10], Step [120/179], Loss: 0.1010\nEpoch [9/10], Step [130/179], Loss: 0.0172\nEpoch [9/10], Step [140/179], Loss: 0.0558\nEpoch [9/10], Step [150/179], Loss: 0.0670\nEpoch [9/10], Step [160/179], Loss: 0.0486\nEpoch [9/10], Step [170/179], Loss: 0.1381\n--- Hết Epoch 9 ---\nTrain Loss: 0.0647\nDev Loss: 0.5839 | Dev Acc: 86.42% | Dev F1: 74.57%\n------------------------------\nEpoch [10/10], Step [10/179], Loss: 0.0263\nEpoch [10/10], Step [20/179], Loss: 0.0825\nEpoch [10/10], Step [30/179], Loss: 0.0137\nEpoch [10/10], Step [40/179], Loss: 0.0754\nEpoch [10/10], Step [50/179], Loss: 0.0200\nEpoch [10/10], Step [60/179], Loss: 0.0167\nEpoch [10/10], Step [70/179], Loss: 0.0831\nEpoch [10/10], Step [80/179], Loss: 0.0981\nEpoch [10/10], Step [90/179], Loss: 0.0130\nEpoch [10/10], Step [100/179], Loss: 0.0085\nEpoch [10/10], Step [110/179], Loss: 0.0193\nEpoch [10/10], Step [120/179], Loss: 0.0400\nEpoch [10/10], Step [130/179], Loss: 0.0365\nEpoch [10/10], Step [140/179], Loss: 0.0180\nEpoch [10/10], Step [150/179], Loss: 0.0943\nEpoch [10/10], Step [160/179], Loss: 0.0165\nEpoch [10/10], Step [170/179], Loss: 0.1438\n--- Hết Epoch 10 ---\nTrain Loss: 0.0509\nDev Loss: 0.6716 | Dev Acc: 86.17% | Dev F1: 75.03%\n------------------------------\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.6611\nTest Accuracy: 86.26%\nTest F1-Score: 73.74%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Bài 3","metadata":{}},{"cell_type":"markdown","source":"## Import module","metadata":{}},{"cell_type":"code","source":"from PhoNer import Vocab, PhoNER, collate_fn \nfrom seq2seq import Encoder, Decoder, Seq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:08:12.938348Z","iopub.execute_input":"2025-11-26T08:08:12.938565Z","iopub.status.idle":"2025-11-26T08:08:12.945735Z","shell.execute_reply.started":"2025-11-26T08:08:12.938547Z","shell.execute_reply":"2025-11-26T08:08:12.944994Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Cấu hình","metadata":{}},{"cell_type":"code","source":"# --- CONFIG ---\nBATCH_SIZE = 32 \nENC_EMB_DIM = 100\nDEC_EMB_DIM = 100\nHIDDEN_DIM = 256 \nN_LAYERS = 5       \nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\nEPOCHS = 10\nLEARNING_RATE = 0.001\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:08:12.946504Z","iopub.execute_input":"2025-11-26T08:08:12.946741Z","iopub.status.idle":"2025-11-26T08:08:12.962335Z","shell.execute_reply.started":"2025-11-26T08:08:12.946717Z","shell.execute_reply":"2025-11-26T08:08:12.961693Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Train, Eval bài 3","metadata":{}},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, tag_pad_idx):\n    model.eval()\n    epoch_loss = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            src = batch['input_ids'].to(DEVICE)\n            trg = batch['label'].to(DEVICE)\n\n            output = model(src, trg, 0) \n            \n            # output: [batch, seq_len, output_dim]\n            # trg: [batch, seq_len]\n            \n            # Reshape để tính loss\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim) # Bỏ token đầu\n            trg = trg[:, 1:].reshape(-1)                   # Bỏ token đầu\n            \n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            \n            preds = output.argmax(dim=1)\n            \n            preds_list = preds.cpu().numpy()\n            trg_list = trg.cpu().numpy()\n\n            valid_indices = np.where(trg_list != tag_pad_idx)[0]\n            \n            all_preds.extend(preds_list[valid_indices])\n            all_labels.extend(trg_list[valid_indices])\n            \n    avg_loss = epoch_loss / len(iterator)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    return avg_loss, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:08:12.963080Z","iopub.execute_input":"2025-11-26T08:08:12.963389Z","iopub.status.idle":"2025-11-26T08:08:12.986380Z","shell.execute_reply.started":"2025-11-26T08:08:12.963365Z","shell.execute_reply":"2025-11-26T08:08:12.985746Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        src = batch['input_ids'].to(DEVICE)\n        trg = batch['label'].to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        # Reshape output và trg để tính CrossEntropy\n        \n        # output shape: [batch, seq_len, n_tags] -> flatten -> [batch * seq, n_tags]\n        # trg shape: [batch, seq_len] -> flatten -> [batch * seq]\n\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        \n        # tránh exploding gradient với LSTM nhiều lớp\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        if (i+1) % 10 == 0:\n             print(f\"Step {i+1}/{len(iterator)} | Loss: {loss.item():.4f}\")\n        \n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:08:12.987094Z","iopub.execute_input":"2025-11-26T08:08:12.987248Z","iopub.status.idle":"2025-11-26T08:08:13.004339Z","shell.execute_reply.started":"2025-11-26T08:08:12.987236Z","shell.execute_reply":"2025-11-26T08:08:13.003859Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Main bài 3","metadata":{}},{"cell_type":"code","source":"def main():\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/PhoNER' \n    \n    print(\"Đang đọc dữ liệu...\")\n    vocab = Vocab(base_path)\n    print(f\"Vocab size: {vocab.len} | Tags: {vocab.n_labels}\")\n    \n    train_data = PhoNER(os.path.join(base_path, 'train_word.json'), vocab)\n    dev_data = PhoNER(os.path.join(base_path, 'dev_word.json'), vocab)\n    test_data = PhoNER(os.path.join(base_path, 'test_word.json'), vocab)\n    \n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    \n    # Model\n    input_dim = vocab.len\n    output_dim = vocab.n_labels\n    \n    enc = Encoder(input_dim, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n    dec = Decoder(output_dim, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n    \n    # Init weights (giúp hội tụ tốt hơn)\n    def init_weights(m):\n        for name, param in m.named_parameters():\n            nn.init.uniform_(param.data, -0.08, 0.08)\n    model.apply(init_weights)\n\n    print(f'Mô hình có {sum(p.numel() for p in model.parameters() if p.requires_grad):,} tham số train được.')\n\n    # 3. Optimizer & Loss\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # ignore_index=-100 để không tính loss cho phần padding\n    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    \n    # 4. Training \n    print(\"Bắt đầu huấn luyện...\")\n    best_valid_loss = float('inf')\n    \n    for epoch in range(EPOCHS):\n        train_loss = train(model, train_loader, optimizer, criterion, clip=1)\n        valid_loss, valid_f1 = evaluate(model, dev_loader, criterion, tag_pad_idx=-100)\n        \n        print(f'Epoch: {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_loss:.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1: {valid_f1*100:.2f}%')\n        \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'phoner_seq2seq.pt')\n    \n    # 5. Test\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    model.load_state_dict(torch.load('phoner_seq2seq.pt'))\n    test_loss, test_f1 = evaluate(model, test_loader, criterion, tag_pad_idx=-100)\n    print(f'Test Loss: {test_loss:.3f} | Test F1: {test_f1*100:.2f}%')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T08:08:13.005092Z","iopub.execute_input":"2025-11-26T08:08:13.005301Z","iopub.status.idle":"2025-11-26T08:13:07.919579Z","shell.execute_reply.started":"2025-11-26T08:08:13.005288Z","shell.execute_reply":"2025-11-26T08:13:07.918907Z"}},"outputs":[{"name":"stdout","text":"Đang đọc dữ liệu...\n--- Đang quét dữ liệu tại: /kaggle/working/DS201_Lab03-RNN/PhoNER ---\n-> Đã xây dựng Vocab: 7306 từ, 21 nhãn (bao gồm pad).\nVocab size: 7306 | Tags: 21\n-> Đã tải 5027 mẫu từ train_word.json\n-> Đã tải 2000 mẫu từ dev_word.json\n-> Đã tải 3000 mẫu từ test_word.json\nMô hình có 5,681,969 tham số train được.\nBắt đầu huấn luyện...\nStep 10/158 | Loss: 1.1429\nStep 20/158 | Loss: 0.9231\nStep 30/158 | Loss: 0.9196\nStep 40/158 | Loss: 0.9318\nStep 50/158 | Loss: 0.8229\nStep 60/158 | Loss: 0.6711\nStep 70/158 | Loss: 0.6735\nStep 80/158 | Loss: 0.6122\nStep 90/158 | Loss: 0.3800\nStep 100/158 | Loss: 0.5015\nStep 110/158 | Loss: 0.4717\nStep 120/158 | Loss: 0.4660\nStep 130/158 | Loss: 0.5215\nStep 140/158 | Loss: 0.5736\nStep 150/158 | Loss: 0.5523\nEpoch: 01\n\tTrain Loss: 0.723\n\t Val. Loss: 1.692 |  Val. F1: 2.40%\nStep 10/158 | Loss: 0.5880\nStep 20/158 | Loss: 0.5398\nStep 30/158 | Loss: 0.5084\nStep 40/158 | Loss: 0.6862\nStep 50/158 | Loss: 0.2930\nStep 60/158 | Loss: 0.6108\nStep 70/158 | Loss: 0.6750\nStep 80/158 | Loss: 0.2930\nStep 90/158 | Loss: 0.5781\nStep 100/158 | Loss: 0.3591\nStep 110/158 | Loss: 0.5261\nStep 120/158 | Loss: 0.5130\nStep 130/158 | Loss: 0.4560\nStep 140/158 | Loss: 0.6526\nStep 150/158 | Loss: 0.5249\nEpoch: 02\n\tTrain Loss: 0.550\n\t Val. Loss: 1.704 |  Val. F1: 2.40%\nStep 10/158 | Loss: 0.6357\nStep 20/158 | Loss: 0.6072\nStep 30/158 | Loss: 0.5139\nStep 40/158 | Loss: 0.5227\nStep 50/158 | Loss: 0.9103\nStep 60/158 | Loss: 0.4959\nStep 70/158 | Loss: 0.6192\nStep 80/158 | Loss: 0.5290\nStep 90/158 | Loss: 0.4763\nStep 100/158 | Loss: 0.4324\nStep 110/158 | Loss: 0.5258\nStep 120/158 | Loss: 0.6636\nStep 130/158 | Loss: 0.5661\nStep 140/158 | Loss: 0.6747\nStep 150/158 | Loss: 0.6717\nEpoch: 03\n\tTrain Loss: 0.558\n\t Val. Loss: 1.445 |  Val. F1: 2.40%\nStep 10/158 | Loss: 0.6044\nStep 20/158 | Loss: 0.6637\nStep 30/158 | Loss: 0.5830\nStep 40/158 | Loss: 0.8680\nStep 50/158 | Loss: 0.5942\nStep 60/158 | Loss: 0.5150\nStep 70/158 | Loss: 0.5872\nStep 80/158 | Loss: 0.6203\nStep 90/158 | Loss: 0.6038\nStep 100/158 | Loss: 0.5041\nStep 110/158 | Loss: 0.5248\nStep 120/158 | Loss: 0.4746\nStep 130/158 | Loss: 0.7108\nStep 140/158 | Loss: 0.5812\nStep 150/158 | Loss: 0.6622\nEpoch: 04\n\tTrain Loss: 0.531\n\t Val. Loss: 1.672 |  Val. F1: 2.40%\nStep 10/158 | Loss: 0.4820\nStep 20/158 | Loss: 0.7763\nStep 30/158 | Loss: 0.5055\nStep 40/158 | Loss: 0.6036\nStep 50/158 | Loss: 0.6783\nStep 60/158 | Loss: 0.5893\nStep 70/158 | Loss: 0.4725\nStep 80/158 | Loss: 0.4959\nStep 90/158 | Loss: 0.3352\nStep 100/158 | Loss: 0.4467\nStep 110/158 | Loss: 0.5599\nStep 120/158 | Loss: 0.5638\nStep 130/158 | Loss: 0.5859\nStep 140/158 | Loss: 0.4789\nStep 150/158 | Loss: 0.5798\nEpoch: 05\n\tTrain Loss: 0.539\n\t Val. Loss: 1.659 |  Val. F1: 2.40%\nStep 10/158 | Loss: 0.4778\nStep 20/158 | Loss: 0.4744\nStep 30/158 | Loss: 0.3120\nStep 40/158 | Loss: 0.5026\nStep 50/158 | Loss: 0.5059\nStep 60/158 | Loss: 0.5717\nStep 70/158 | Loss: 0.5729\nStep 80/158 | Loss: 0.5407\nStep 90/158 | Loss: 0.4963\nStep 100/158 | Loss: 0.5398\nStep 110/158 | Loss: 0.5298\nStep 120/158 | Loss: 0.5511\nStep 130/158 | Loss: 0.4909\nStep 140/158 | Loss: 0.4348\nStep 150/158 | Loss: 0.5027\nEpoch: 06\n\tTrain Loss: 0.524\n\t Val. Loss: 1.572 |  Val. F1: 2.40%\nStep 10/158 | Loss: 0.4736\nStep 20/158 | Loss: 0.5860\nStep 30/158 | Loss: 0.5456\nStep 40/158 | Loss: 0.6476\nStep 50/158 | Loss: 0.3822\nStep 60/158 | Loss: 0.4336\nStep 70/158 | Loss: 0.6560\nStep 80/158 | Loss: 0.5608\nStep 90/158 | Loss: 0.7015\nStep 100/158 | Loss: 0.5791\nStep 110/158 | Loss: 0.6134\nStep 120/158 | Loss: 0.5147\nStep 130/158 | Loss: 0.3872\nStep 140/158 | Loss: 0.3958\nStep 150/158 | Loss: 0.4035\nEpoch: 07\n\tTrain Loss: 0.521\n\t Val. Loss: 1.514 |  Val. F1: 3.03%\nStep 10/158 | Loss: 0.3644\nStep 20/158 | Loss: 0.4138\nStep 30/158 | Loss: 0.5034\nStep 40/158 | Loss: 0.5364\nStep 50/158 | Loss: 0.5041\nStep 60/158 | Loss: 0.5138\nStep 70/158 | Loss: 0.3395\nStep 80/158 | Loss: 0.4843\nStep 90/158 | Loss: 0.3114\nStep 100/158 | Loss: 0.4186\nStep 110/158 | Loss: 0.4431\nStep 120/158 | Loss: 0.4953\nStep 130/158 | Loss: 0.4029\nStep 140/158 | Loss: 0.2348\nStep 150/158 | Loss: 0.6892\nEpoch: 08\n\tTrain Loss: 0.474\n\t Val. Loss: 1.642 |  Val. F1: 2.71%\nStep 10/158 | Loss: 0.2669\nStep 20/158 | Loss: 0.5288\nStep 30/158 | Loss: 0.5812\nStep 40/158 | Loss: 0.5303\nStep 50/158 | Loss: 0.3081\nStep 60/158 | Loss: 0.5491\nStep 70/158 | Loss: 0.5905\nStep 80/158 | Loss: 0.4224\nStep 90/158 | Loss: 0.5995\nStep 100/158 | Loss: 0.4405\nStep 110/158 | Loss: 0.4777\nStep 120/158 | Loss: 0.5966\nStep 130/158 | Loss: 0.3599\nStep 140/158 | Loss: 0.4815\nStep 150/158 | Loss: 0.4361\nEpoch: 09\n\tTrain Loss: 0.477\n\t Val. Loss: 1.589 |  Val. F1: 2.71%\nStep 10/158 | Loss: 0.3096\nStep 20/158 | Loss: 0.2808\nStep 30/158 | Loss: 0.4771\nStep 40/158 | Loss: 0.3424\nStep 50/158 | Loss: 0.5882\nStep 60/158 | Loss: 0.3644\nStep 70/158 | Loss: 0.3682\nStep 80/158 | Loss: 0.3429\nStep 90/158 | Loss: 0.5955\nStep 100/158 | Loss: 0.3470\nStep 110/158 | Loss: 0.5530\nStep 120/158 | Loss: 0.3385\nStep 130/158 | Loss: 0.4592\nStep 140/158 | Loss: 0.4511\nStep 150/158 | Loss: 0.3136\nEpoch: 10\n\tTrain Loss: 0.463\n\t Val. Loss: 1.636 |  Val. F1: 2.99%\n\nĐang đánh giá trên tập Test...\nTest Loss: 1.455 | Test F1: 2.23%\n","output_type":"stream"}],"execution_count":13}]}