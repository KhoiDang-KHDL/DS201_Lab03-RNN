{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Git","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/KhoiDang-KHDL/DS201_Lab03-RNN\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.192833Z","iopub.execute_input":"2025-11-27T10:41:56.193290Z","iopub.status.idle":"2025-11-27T10:41:56.333444Z","shell.execute_reply.started":"2025-11-27T10:41:56.193266Z","shell.execute_reply":"2025-11-27T10:41:56.332756Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'DS201_Lab03-RNN' already exists and is not an empty directory.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/DS201_Lab03-RNN') \n\n# Check thư mục\nimport os\nprint(os.listdir('/kaggle/working/DS201_Lab03-RNN'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.334891Z","iopub.execute_input":"2025-11-27T10:41:56.335142Z","iopub.status.idle":"2025-11-27T10:41:56.340089Z","shell.execute_reply.started":"2025-11-27T10:41:56.335120Z","shell.execute_reply":"2025-11-27T10:41:56.339394Z"}},"outputs":[{"name":"stdout","text":"['seq2seq.py', '__pycache__', 'UIT-VSFC-20251121T013658Z-1-001', 'uit_vsfc.py', 'PhoNER', 'PhoNer.py', 'gru.py', 'lstm.py', '.git', 'main.ipynb']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Thư viện","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport os\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\nimport uit_vsfc\n\nfrom uit_vsfc import Vocab, UIT_VSFC, collate_fn\nfrom lstm import LSTMModel\nfrom gru import GRUModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.340969Z","iopub.execute_input":"2025-11-27T10:41:56.341221Z","iopub.status.idle":"2025-11-27T10:41:56.360280Z","shell.execute_reply.started":"2025-11-27T10:41:56.341202Z","shell.execute_reply":"2025-11-27T10:41:56.359764Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Cấu hình cho bài 1,2","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nHIDDEN_SIZE = 128  \nN_LAYERS = 2\nLEARNING_RATE = 0.0001\nEPOCHS = 10\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.361897Z","iopub.execute_input":"2025-11-27T10:41:56.362197Z","iopub.status.idle":"2025-11-27T10:41:56.386000Z","shell.execute_reply.started":"2025-11-27T10:41:56.362173Z","shell.execute_reply":"2025-11-27T10:41:56.385443Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Train, Eval bài 1,2","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n            \n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            # Lấy nhãn dự đoán\n            _, predicted = torch.max(outputs, 1)\n            \n            # Chuyển về cpu để tính trong sklearn\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    avg_loss = total_loss / len(dataloader)\n    \n    # Tính Accuracy và F1-Score\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    return avg_loss, acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.386648Z","iopub.execute_input":"2025-11-27T10:41:56.386835Z","iopub.status.idle":"2025-11-27T10:41:56.403901Z","shell.execute_reply.started":"2025-11-27T10:41:56.386822Z","shell.execute_reply":"2025-11-27T10:41:56.403355Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def train(model, train_loader, dev_loader, optimizer, criterion, epochs):\n    print(f\"Bắt đầu huấn luyện trên thiết bị: {DEVICE}\")\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        \n        for i, batch in enumerate(train_loader):\n            input_ids = batch['input_ids'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n        dev_loss, dev_acc, dev_f1 = evaluate(model, dev_loader, criterion)\n        print(f\"--- Hết Epoch {epoch+1} ---\")\n        print(f\"Train Loss: {running_loss/len(train_loader):.4f}\")\n        print(f\"Dev Loss: {dev_loss:.4f} | Dev Acc: {dev_acc*100:.2f}% | Dev F1: {dev_f1*100:.2f}%\")\n        print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.404544Z","iopub.execute_input":"2025-11-27T10:41:56.404847Z","iopub.status.idle":"2025-11-27T10:41:56.422496Z","shell.execute_reply.started":"2025-11-27T10:41:56.404830Z","shell.execute_reply":"2025-11-27T10:41:56.421988Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Bài 1: main","metadata":{}},{"cell_type":"code","source":"def main():\n\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/UIT-VSFC-20251121T013658Z-1-001/UIT-VSFC'\n    \n    train_path = os.path.join(base_path, 'UIT-VSFC-train.json')\n    dev_path = os.path.join(base_path, 'UIT-VSFC-dev.json')\n    test_path = os.path.join(base_path, 'UIT-VSFC-test.json')\n\n    print(\"Đang xây dựng bộ từ điển (Vocab):\")\n    vocab = Vocab(base_path)\n    print(f\"Kích thước từ điển: {vocab.len}\")\n\n    print(\"Đang tải dữ liệu...\")\n    train_dataset = UIT_VSFC(train_path, vocab)\n    dev_dataset = UIT_VSFC(dev_path, vocab)\n    test_dataset = UIT_VSFC(test_path, vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # --- MODEL ---\n    model = LSTMModel(\n        vocab_size=vocab.len,\n        hidden_size=HIDDEN_SIZE,\n        embedding_dim=128,\n        n_layers=N_LAYERS,\n        n_labels=vocab.n_labels,\n        padding_idx=vocab.w2i[vocab.pad]\n    ).to(DEVICE)\n\n    # ---  OPTIMIZER, LOSS ---\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train(model, train_loader, dev_loader, optimizer, criterion, EPOCHS)\n\n    # Evaluate\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"Test F1-Score: {test_f1*100:.2f}%\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:41:56.423174Z","iopub.execute_input":"2025-11-27T10:41:56.423380Z","iopub.status.idle":"2025-11-27T10:42:15.042872Z","shell.execute_reply.started":"2025-11-27T10:41:56.423365Z","shell.execute_reply":"2025-11-27T10:42:15.042219Z"}},"outputs":[{"name":"stdout","text":"Đang xây dựng bộ từ điển (Vocab):\nKích thước từ điển: 2848\nĐang tải dữ liệu...\nBắt đầu huấn luyện trên thiết bị: cuda\nEpoch [1/10], Step [10/179], Loss: 1.3549\nEpoch [1/10], Step [20/179], Loss: 1.2920\nEpoch [1/10], Step [30/179], Loss: 1.2046\nEpoch [1/10], Step [40/179], Loss: 1.1101\nEpoch [1/10], Step [50/179], Loss: 0.9714\nEpoch [1/10], Step [60/179], Loss: 0.7937\nEpoch [1/10], Step [70/179], Loss: 0.6979\nEpoch [1/10], Step [80/179], Loss: 0.7561\nEpoch [1/10], Step [90/179], Loss: 0.7496\nEpoch [1/10], Step [100/179], Loss: 0.8525\nEpoch [1/10], Step [110/179], Loss: 0.6517\nEpoch [1/10], Step [120/179], Loss: 0.7445\nEpoch [1/10], Step [130/179], Loss: 0.5005\nEpoch [1/10], Step [140/179], Loss: 0.5608\nEpoch [1/10], Step [150/179], Loss: 0.5385\nEpoch [1/10], Step [160/179], Loss: 0.5815\nEpoch [1/10], Step [170/179], Loss: 0.5770\n--- Hết Epoch 1 ---\nTrain Loss: 0.8384\nDev Loss: 0.5777 | Dev Acc: 78.71% | Dev F1: 36.39%\n------------------------------\nEpoch [2/10], Step [10/179], Loss: 0.5678\nEpoch [2/10], Step [20/179], Loss: 0.5032\nEpoch [2/10], Step [30/179], Loss: 0.7371\nEpoch [2/10], Step [40/179], Loss: 0.5931\nEpoch [2/10], Step [50/179], Loss: 0.5476\nEpoch [2/10], Step [60/179], Loss: 0.4037\nEpoch [2/10], Step [70/179], Loss: 0.4352\nEpoch [2/10], Step [80/179], Loss: 0.3959\nEpoch [2/10], Step [90/179], Loss: 0.5801\nEpoch [2/10], Step [100/179], Loss: 0.4632\nEpoch [2/10], Step [110/179], Loss: 0.3057\nEpoch [2/10], Step [120/179], Loss: 0.3859\nEpoch [2/10], Step [130/179], Loss: 0.5844\nEpoch [2/10], Step [140/179], Loss: 0.5847\nEpoch [2/10], Step [150/179], Loss: 0.5939\nEpoch [2/10], Step [160/179], Loss: 0.3941\nEpoch [2/10], Step [170/179], Loss: 0.4533\n--- Hết Epoch 2 ---\nTrain Loss: 0.5214\nDev Loss: 0.4799 | Dev Acc: 80.80% | Dev F1: 38.30%\n------------------------------\nEpoch [3/10], Step [10/179], Loss: 0.5405\nEpoch [3/10], Step [20/179], Loss: 0.5262\nEpoch [3/10], Step [30/179], Loss: 0.4230\nEpoch [3/10], Step [40/179], Loss: 0.3087\nEpoch [3/10], Step [50/179], Loss: 0.4515\nEpoch [3/10], Step [60/179], Loss: 0.5069\nEpoch [3/10], Step [70/179], Loss: 0.3171\nEpoch [3/10], Step [80/179], Loss: 0.4674\nEpoch [3/10], Step [90/179], Loss: 0.3977\nEpoch [3/10], Step [100/179], Loss: 0.5248\nEpoch [3/10], Step [110/179], Loss: 0.4800\nEpoch [3/10], Step [120/179], Loss: 0.3735\nEpoch [3/10], Step [130/179], Loss: 0.4349\nEpoch [3/10], Step [140/179], Loss: 0.4859\nEpoch [3/10], Step [150/179], Loss: 0.3591\nEpoch [3/10], Step [160/179], Loss: 0.5547\nEpoch [3/10], Step [170/179], Loss: 0.3885\n--- Hết Epoch 3 ---\nTrain Loss: 0.4413\nDev Loss: 0.4174 | Dev Acc: 84.08% | Dev F1: 58.48%\n------------------------------\nEpoch [4/10], Step [10/179], Loss: 0.3319\nEpoch [4/10], Step [20/179], Loss: 0.3192\nEpoch [4/10], Step [30/179], Loss: 0.4560\nEpoch [4/10], Step [40/179], Loss: 0.3509\nEpoch [4/10], Step [50/179], Loss: 0.3947\nEpoch [4/10], Step [60/179], Loss: 0.3470\nEpoch [4/10], Step [70/179], Loss: 0.4013\nEpoch [4/10], Step [80/179], Loss: 0.3813\nEpoch [4/10], Step [90/179], Loss: 0.4850\nEpoch [4/10], Step [100/179], Loss: 0.4214\nEpoch [4/10], Step [110/179], Loss: 0.3191\nEpoch [4/10], Step [120/179], Loss: 0.3873\nEpoch [4/10], Step [130/179], Loss: 0.4251\nEpoch [4/10], Step [140/179], Loss: 0.4506\nEpoch [4/10], Step [150/179], Loss: 0.3630\nEpoch [4/10], Step [160/179], Loss: 0.3094\nEpoch [4/10], Step [170/179], Loss: 0.4137\n--- Hết Epoch 4 ---\nTrain Loss: 0.3850\nDev Loss: 0.3827 | Dev Acc: 85.53% | Dev F1: 63.75%\n------------------------------\nEpoch [5/10], Step [10/179], Loss: 0.3621\nEpoch [5/10], Step [20/179], Loss: 0.4030\nEpoch [5/10], Step [30/179], Loss: 0.2567\nEpoch [5/10], Step [40/179], Loss: 0.4891\nEpoch [5/10], Step [50/179], Loss: 0.4639\nEpoch [5/10], Step [60/179], Loss: 0.4376\nEpoch [5/10], Step [70/179], Loss: 0.4368\nEpoch [5/10], Step [80/179], Loss: 0.2357\nEpoch [5/10], Step [90/179], Loss: 0.3823\nEpoch [5/10], Step [100/179], Loss: 0.3094\nEpoch [5/10], Step [110/179], Loss: 0.2587\nEpoch [5/10], Step [120/179], Loss: 0.3259\nEpoch [5/10], Step [130/179], Loss: 0.5294\nEpoch [5/10], Step [140/179], Loss: 0.2943\nEpoch [5/10], Step [150/179], Loss: 0.2560\nEpoch [5/10], Step [160/179], Loss: 0.3819\nEpoch [5/10], Step [170/179], Loss: 0.3702\n--- Hết Epoch 5 ---\nTrain Loss: 0.3525\nDev Loss: 0.3711 | Dev Acc: 86.36% | Dev F1: 67.28%\n------------------------------\nEpoch [6/10], Step [10/179], Loss: 0.2683\nEpoch [6/10], Step [20/179], Loss: 0.1884\nEpoch [6/10], Step [30/179], Loss: 0.2367\nEpoch [6/10], Step [40/179], Loss: 0.3899\nEpoch [6/10], Step [50/179], Loss: 0.3515\nEpoch [6/10], Step [60/179], Loss: 0.2393\nEpoch [6/10], Step [70/179], Loss: 0.4781\nEpoch [6/10], Step [80/179], Loss: 0.2601\nEpoch [6/10], Step [90/179], Loss: 0.2610\nEpoch [6/10], Step [100/179], Loss: 0.1952\nEpoch [6/10], Step [110/179], Loss: 0.4329\nEpoch [6/10], Step [120/179], Loss: 0.3895\nEpoch [6/10], Step [130/179], Loss: 0.4025\nEpoch [6/10], Step [140/179], Loss: 0.3286\nEpoch [6/10], Step [150/179], Loss: 0.3564\nEpoch [6/10], Step [160/179], Loss: 0.3364\nEpoch [6/10], Step [170/179], Loss: 0.3087\n--- Hết Epoch 6 ---\nTrain Loss: 0.3300\nDev Loss: 0.3622 | Dev Acc: 86.36% | Dev F1: 71.38%\n------------------------------\nEpoch [7/10], Step [10/179], Loss: 0.3114\nEpoch [7/10], Step [20/179], Loss: 0.2564\nEpoch [7/10], Step [30/179], Loss: 0.3247\nEpoch [7/10], Step [40/179], Loss: 0.3272\nEpoch [7/10], Step [50/179], Loss: 0.3830\nEpoch [7/10], Step [60/179], Loss: 0.2623\nEpoch [7/10], Step [70/179], Loss: 0.1801\nEpoch [7/10], Step [80/179], Loss: 0.3459\nEpoch [7/10], Step [90/179], Loss: 0.5116\nEpoch [7/10], Step [100/179], Loss: 0.4904\nEpoch [7/10], Step [110/179], Loss: 0.2913\nEpoch [7/10], Step [120/179], Loss: 0.1614\nEpoch [7/10], Step [130/179], Loss: 0.3259\nEpoch [7/10], Step [140/179], Loss: 0.1690\nEpoch [7/10], Step [150/179], Loss: 0.2923\nEpoch [7/10], Step [160/179], Loss: 0.3222\nEpoch [7/10], Step [170/179], Loss: 0.3671\n--- Hết Epoch 7 ---\nTrain Loss: 0.3099\nDev Loss: 0.3663 | Dev Acc: 86.23% | Dev F1: 71.20%\n------------------------------\nEpoch [8/10], Step [10/179], Loss: 0.3515\nEpoch [8/10], Step [20/179], Loss: 0.4374\nEpoch [8/10], Step [30/179], Loss: 0.4174\nEpoch [8/10], Step [40/179], Loss: 0.2647\nEpoch [8/10], Step [50/179], Loss: 0.2458\nEpoch [8/10], Step [60/179], Loss: 0.3113\nEpoch [8/10], Step [70/179], Loss: 0.2830\nEpoch [8/10], Step [80/179], Loss: 0.3589\nEpoch [8/10], Step [90/179], Loss: 0.2492\nEpoch [8/10], Step [100/179], Loss: 0.3184\nEpoch [8/10], Step [110/179], Loss: 0.3402\nEpoch [8/10], Step [120/179], Loss: 0.2376\nEpoch [8/10], Step [130/179], Loss: 0.3847\nEpoch [8/10], Step [140/179], Loss: 0.3152\nEpoch [8/10], Step [150/179], Loss: 0.3368\nEpoch [8/10], Step [160/179], Loss: 0.2455\nEpoch [8/10], Step [170/179], Loss: 0.5117\n--- Hết Epoch 8 ---\nTrain Loss: 0.2956\nDev Loss: 0.3752 | Dev Acc: 85.47% | Dev F1: 70.74%\n------------------------------\nEpoch [9/10], Step [10/179], Loss: 0.1873\nEpoch [9/10], Step [20/179], Loss: 0.1836\nEpoch [9/10], Step [30/179], Loss: 0.3531\nEpoch [9/10], Step [40/179], Loss: 0.2550\nEpoch [9/10], Step [50/179], Loss: 0.2403\nEpoch [9/10], Step [60/179], Loss: 0.1964\nEpoch [9/10], Step [70/179], Loss: 0.2707\nEpoch [9/10], Step [80/179], Loss: 0.1965\nEpoch [9/10], Step [90/179], Loss: 0.2562\nEpoch [9/10], Step [100/179], Loss: 0.1605\nEpoch [9/10], Step [110/179], Loss: 0.2254\nEpoch [9/10], Step [120/179], Loss: 0.2467\nEpoch [9/10], Step [130/179], Loss: 0.2759\nEpoch [9/10], Step [140/179], Loss: 0.3975\nEpoch [9/10], Step [150/179], Loss: 0.1612\nEpoch [9/10], Step [160/179], Loss: 0.3088\nEpoch [9/10], Step [170/179], Loss: 0.2437\n--- Hết Epoch 9 ---\nTrain Loss: 0.2820\nDev Loss: 0.3596 | Dev Acc: 87.30% | Dev F1: 75.00%\n------------------------------\nEpoch [10/10], Step [10/179], Loss: 0.2615\nEpoch [10/10], Step [20/179], Loss: 0.1273\nEpoch [10/10], Step [30/179], Loss: 0.1833\nEpoch [10/10], Step [40/179], Loss: 0.1031\nEpoch [10/10], Step [50/179], Loss: 0.2926\nEpoch [10/10], Step [60/179], Loss: 0.1970\nEpoch [10/10], Step [70/179], Loss: 0.3057\nEpoch [10/10], Step [80/179], Loss: 0.2283\nEpoch [10/10], Step [90/179], Loss: 0.4199\nEpoch [10/10], Step [100/179], Loss: 0.3717\nEpoch [10/10], Step [110/179], Loss: 0.2103\nEpoch [10/10], Step [120/179], Loss: 0.2801\nEpoch [10/10], Step [130/179], Loss: 0.1467\nEpoch [10/10], Step [140/179], Loss: 0.1495\nEpoch [10/10], Step [150/179], Loss: 0.1636\nEpoch [10/10], Step [160/179], Loss: 0.2572\nEpoch [10/10], Step [170/179], Loss: 0.2744\n--- Hết Epoch 10 ---\nTrain Loss: 0.2627\nDev Loss: 0.3637 | Dev Acc: 87.30% | Dev F1: 74.69%\n------------------------------\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.3684\nTest Accuracy: 86.13%\nTest F1-Score: 73.19%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Bài 2: main","metadata":{}},{"cell_type":"code","source":"def main():\n\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/UIT-VSFC-20251121T013658Z-1-001/UIT-VSFC'\n    \n    train_path = os.path.join(base_path, 'UIT-VSFC-train.json')\n    dev_path = os.path.join(base_path, 'UIT-VSFC-dev.json')\n    test_path = os.path.join(base_path, 'UIT-VSFC-test.json')\n\n    print(\"Đang xây dựng bộ từ điển (Vocab)...\")\n    vocab = Vocab(base_path)\n    print(f\"Kích thước từ điển: {vocab.len}\")\n\n    print(\"Đang tải dữ liệu...\")\n    train_dataset = UIT_VSFC(train_path, vocab)\n    dev_dataset = UIT_VSFC(dev_path, vocab)\n    test_dataset = UIT_VSFC(test_path, vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # ---  MODEL ---\n    model = GRUModel(\n        vocab_size=vocab.len,\n        embedding_dim=128,\n        hidden_size=HIDDEN_SIZE,\n        n_layers=N_LAYERS,\n        n_labels=vocab.n_labels,\n        padding_idx=vocab.w2i[vocab.pad]\n    ).to(DEVICE)\n\n    # ---  OPTIMIZER, LOSS ---\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train(model, train_loader, dev_loader, optimizer, criterion, EPOCHS)\n    \n    # Evaluate\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"Test F1-Score: {test_f1*100:.2f}%\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:15.043753Z","iopub.execute_input":"2025-11-27T10:42:15.044470Z","iopub.status.idle":"2025-11-27T10:42:29.628134Z","shell.execute_reply.started":"2025-11-27T10:42:15.044450Z","shell.execute_reply":"2025-11-27T10:42:29.627307Z"}},"outputs":[{"name":"stdout","text":"Đang xây dựng bộ từ điển (Vocab)...\nKích thước từ điển: 2848\nĐang tải dữ liệu...\nBắt đầu huấn luyện trên thiết bị: cuda\nEpoch [1/10], Step [10/179], Loss: 1.1843\nEpoch [1/10], Step [20/179], Loss: 1.0253\nEpoch [1/10], Step [30/179], Loss: 0.8681\nEpoch [1/10], Step [40/179], Loss: 0.9069\nEpoch [1/10], Step [50/179], Loss: 0.8451\nEpoch [1/10], Step [60/179], Loss: 0.8173\nEpoch [1/10], Step [70/179], Loss: 0.7996\nEpoch [1/10], Step [80/179], Loss: 0.7197\nEpoch [1/10], Step [90/179], Loss: 0.9797\nEpoch [1/10], Step [100/179], Loss: 0.8058\nEpoch [1/10], Step [110/179], Loss: 0.6664\nEpoch [1/10], Step [120/179], Loss: 0.7911\nEpoch [1/10], Step [130/179], Loss: 0.5435\nEpoch [1/10], Step [140/179], Loss: 0.5836\nEpoch [1/10], Step [150/179], Loss: 0.4921\nEpoch [1/10], Step [160/179], Loss: 0.7148\nEpoch [1/10], Step [170/179], Loss: 0.4905\n--- Hết Epoch 1 ---\nTrain Loss: 0.7551\nDev Loss: 0.5495 | Dev Acc: 80.04% | Dev F1: 37.70%\n------------------------------\nEpoch [2/10], Step [10/179], Loss: 0.6268\nEpoch [2/10], Step [20/179], Loss: 0.6282\nEpoch [2/10], Step [30/179], Loss: 0.4915\nEpoch [2/10], Step [40/179], Loss: 0.3659\nEpoch [2/10], Step [50/179], Loss: 0.5251\nEpoch [2/10], Step [60/179], Loss: 0.3316\nEpoch [2/10], Step [70/179], Loss: 0.6343\nEpoch [2/10], Step [80/179], Loss: 0.5931\nEpoch [2/10], Step [90/179], Loss: 0.4331\nEpoch [2/10], Step [100/179], Loss: 0.6012\nEpoch [2/10], Step [110/179], Loss: 0.3739\nEpoch [2/10], Step [120/179], Loss: 0.4841\nEpoch [2/10], Step [130/179], Loss: 0.5250\nEpoch [2/10], Step [140/179], Loss: 0.4823\nEpoch [2/10], Step [150/179], Loss: 0.3885\nEpoch [2/10], Step [160/179], Loss: 0.5279\nEpoch [2/10], Step [170/179], Loss: 0.4392\n--- Hết Epoch 2 ---\nTrain Loss: 0.5013\nDev Loss: 0.4650 | Dev Acc: 82.82% | Dev F1: 54.61%\n------------------------------\nEpoch [3/10], Step [10/179], Loss: 0.4501\nEpoch [3/10], Step [20/179], Loss: 0.3042\nEpoch [3/10], Step [30/179], Loss: 0.4415\nEpoch [3/10], Step [40/179], Loss: 0.3321\nEpoch [3/10], Step [50/179], Loss: 0.3368\nEpoch [3/10], Step [60/179], Loss: 0.5709\nEpoch [3/10], Step [70/179], Loss: 0.4659\nEpoch [3/10], Step [80/179], Loss: 0.3877\nEpoch [3/10], Step [90/179], Loss: 0.4726\nEpoch [3/10], Step [100/179], Loss: 0.4475\nEpoch [3/10], Step [110/179], Loss: 0.4206\nEpoch [3/10], Step [120/179], Loss: 0.4502\nEpoch [3/10], Step [130/179], Loss: 0.4505\nEpoch [3/10], Step [140/179], Loss: 0.3224\nEpoch [3/10], Step [150/179], Loss: 0.3862\nEpoch [3/10], Step [160/179], Loss: 0.5142\nEpoch [3/10], Step [170/179], Loss: 0.2789\n--- Hết Epoch 3 ---\nTrain Loss: 0.4191\nDev Loss: 0.4156 | Dev Acc: 84.02% | Dev F1: 60.00%\n------------------------------\nEpoch [4/10], Step [10/179], Loss: 0.4389\nEpoch [4/10], Step [20/179], Loss: 0.4223\nEpoch [4/10], Step [30/179], Loss: 0.5223\nEpoch [4/10], Step [40/179], Loss: 0.3938\nEpoch [4/10], Step [50/179], Loss: 0.3565\nEpoch [4/10], Step [60/179], Loss: 0.4835\nEpoch [4/10], Step [70/179], Loss: 0.3193\nEpoch [4/10], Step [80/179], Loss: 0.5658\nEpoch [4/10], Step [90/179], Loss: 0.3792\nEpoch [4/10], Step [100/179], Loss: 0.4862\nEpoch [4/10], Step [110/179], Loss: 0.3163\nEpoch [4/10], Step [120/179], Loss: 0.4634\nEpoch [4/10], Step [130/179], Loss: 0.4324\nEpoch [4/10], Step [140/179], Loss: 0.4241\nEpoch [4/10], Step [150/179], Loss: 0.2866\nEpoch [4/10], Step [160/179], Loss: 0.2985\nEpoch [4/10], Step [170/179], Loss: 0.4004\n--- Hết Epoch 4 ---\nTrain Loss: 0.3753\nDev Loss: 0.3972 | Dev Acc: 84.40% | Dev F1: 62.59%\n------------------------------\nEpoch [5/10], Step [10/179], Loss: 0.4795\nEpoch [5/10], Step [20/179], Loss: 0.3471\nEpoch [5/10], Step [30/179], Loss: 0.2204\nEpoch [5/10], Step [40/179], Loss: 0.3759\nEpoch [5/10], Step [50/179], Loss: 0.3905\nEpoch [5/10], Step [60/179], Loss: 0.3904\nEpoch [5/10], Step [70/179], Loss: 0.3860\nEpoch [5/10], Step [80/179], Loss: 0.4314\nEpoch [5/10], Step [90/179], Loss: 0.5621\nEpoch [5/10], Step [100/179], Loss: 0.3987\nEpoch [5/10], Step [110/179], Loss: 0.2159\nEpoch [5/10], Step [120/179], Loss: 0.4301\nEpoch [5/10], Step [130/179], Loss: 0.2052\nEpoch [5/10], Step [140/179], Loss: 0.3246\nEpoch [5/10], Step [150/179], Loss: 0.3619\nEpoch [5/10], Step [160/179], Loss: 0.4496\nEpoch [5/10], Step [170/179], Loss: 0.3579\n--- Hết Epoch 5 ---\nTrain Loss: 0.3493\nDev Loss: 0.3780 | Dev Acc: 85.72% | Dev F1: 66.84%\n------------------------------\nEpoch [6/10], Step [10/179], Loss: 0.3742\nEpoch [6/10], Step [20/179], Loss: 0.2809\nEpoch [6/10], Step [30/179], Loss: 0.1650\nEpoch [6/10], Step [40/179], Loss: 0.4340\nEpoch [6/10], Step [50/179], Loss: 0.2690\nEpoch [6/10], Step [60/179], Loss: 0.1173\nEpoch [6/10], Step [70/179], Loss: 0.3940\nEpoch [6/10], Step [80/179], Loss: 0.3353\nEpoch [6/10], Step [90/179], Loss: 0.2984\nEpoch [6/10], Step [100/179], Loss: 0.2193\nEpoch [6/10], Step [110/179], Loss: 0.3345\nEpoch [6/10], Step [120/179], Loss: 0.1442\nEpoch [6/10], Step [130/179], Loss: 0.2889\nEpoch [6/10], Step [140/179], Loss: 0.2683\nEpoch [6/10], Step [150/179], Loss: 0.3242\nEpoch [6/10], Step [160/179], Loss: 0.3212\nEpoch [6/10], Step [170/179], Loss: 0.2916\n--- Hết Epoch 6 ---\nTrain Loss: 0.3268\nDev Loss: 0.3691 | Dev Acc: 85.79% | Dev F1: 68.45%\n------------------------------\nEpoch [7/10], Step [10/179], Loss: 0.4737\nEpoch [7/10], Step [20/179], Loss: 0.3641\nEpoch [7/10], Step [30/179], Loss: 0.1638\nEpoch [7/10], Step [40/179], Loss: 0.2788\nEpoch [7/10], Step [50/179], Loss: 0.1876\nEpoch [7/10], Step [60/179], Loss: 0.4769\nEpoch [7/10], Step [70/179], Loss: 0.3666\nEpoch [7/10], Step [80/179], Loss: 0.2644\nEpoch [7/10], Step [90/179], Loss: 0.3031\nEpoch [7/10], Step [100/179], Loss: 0.2682\nEpoch [7/10], Step [110/179], Loss: 0.2881\nEpoch [7/10], Step [120/179], Loss: 0.3212\nEpoch [7/10], Step [130/179], Loss: 0.3001\nEpoch [7/10], Step [140/179], Loss: 0.2149\nEpoch [7/10], Step [150/179], Loss: 0.2812\nEpoch [7/10], Step [160/179], Loss: 0.2139\nEpoch [7/10], Step [170/179], Loss: 0.3670\n--- Hết Epoch 7 ---\nTrain Loss: 0.3090\nDev Loss: 0.3640 | Dev Acc: 85.85% | Dev F1: 70.66%\n------------------------------\nEpoch [8/10], Step [10/179], Loss: 0.2509\nEpoch [8/10], Step [20/179], Loss: 0.3533\nEpoch [8/10], Step [30/179], Loss: 0.4325\nEpoch [8/10], Step [40/179], Loss: 0.2113\nEpoch [8/10], Step [50/179], Loss: 0.1763\nEpoch [8/10], Step [60/179], Loss: 0.3108\nEpoch [8/10], Step [70/179], Loss: 0.3852\nEpoch [8/10], Step [80/179], Loss: 0.3095\nEpoch [8/10], Step [90/179], Loss: 0.4260\nEpoch [8/10], Step [100/179], Loss: 0.3830\nEpoch [8/10], Step [110/179], Loss: 0.2670\nEpoch [8/10], Step [120/179], Loss: 0.2945\nEpoch [8/10], Step [130/179], Loss: 0.3817\nEpoch [8/10], Step [140/179], Loss: 0.2228\nEpoch [8/10], Step [150/179], Loss: 0.2715\nEpoch [8/10], Step [160/179], Loss: 0.2664\nEpoch [8/10], Step [170/179], Loss: 0.3296\n--- Hết Epoch 8 ---\nTrain Loss: 0.2938\nDev Loss: 0.3636 | Dev Acc: 86.10% | Dev F1: 70.74%\n------------------------------\nEpoch [9/10], Step [10/179], Loss: 0.1818\nEpoch [9/10], Step [20/179], Loss: 0.3979\nEpoch [9/10], Step [30/179], Loss: 0.2408\nEpoch [9/10], Step [40/179], Loss: 0.3300\nEpoch [9/10], Step [50/179], Loss: 0.1014\nEpoch [9/10], Step [60/179], Loss: 0.1955\nEpoch [9/10], Step [70/179], Loss: 0.3535\nEpoch [9/10], Step [80/179], Loss: 0.2695\nEpoch [9/10], Step [90/179], Loss: 0.2669\nEpoch [9/10], Step [100/179], Loss: 0.2940\nEpoch [9/10], Step [110/179], Loss: 0.2357\nEpoch [9/10], Step [120/179], Loss: 0.1738\nEpoch [9/10], Step [130/179], Loss: 0.2918\nEpoch [9/10], Step [140/179], Loss: 0.3029\nEpoch [9/10], Step [150/179], Loss: 0.2519\nEpoch [9/10], Step [160/179], Loss: 0.2528\nEpoch [9/10], Step [170/179], Loss: 0.2709\n--- Hết Epoch 9 ---\nTrain Loss: 0.2824\nDev Loss: 0.3618 | Dev Acc: 86.29% | Dev F1: 71.89%\n------------------------------\nEpoch [10/10], Step [10/179], Loss: 0.2641\nEpoch [10/10], Step [20/179], Loss: 0.1786\nEpoch [10/10], Step [30/179], Loss: 0.2806\nEpoch [10/10], Step [40/179], Loss: 0.2781\nEpoch [10/10], Step [50/179], Loss: 0.3113\nEpoch [10/10], Step [60/179], Loss: 0.2911\nEpoch [10/10], Step [70/179], Loss: 0.2016\nEpoch [10/10], Step [80/179], Loss: 0.3409\nEpoch [10/10], Step [90/179], Loss: 0.2222\nEpoch [10/10], Step [100/179], Loss: 0.1749\nEpoch [10/10], Step [110/179], Loss: 0.2193\nEpoch [10/10], Step [120/179], Loss: 0.2955\nEpoch [10/10], Step [130/179], Loss: 0.1775\nEpoch [10/10], Step [140/179], Loss: 0.2550\nEpoch [10/10], Step [150/179], Loss: 0.1983\nEpoch [10/10], Step [160/179], Loss: 0.3837\nEpoch [10/10], Step [170/179], Loss: 0.2984\n--- Hết Epoch 10 ---\nTrain Loss: 0.2690\nDev Loss: 0.3576 | Dev Acc: 86.29% | Dev F1: 72.85%\n------------------------------\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.3637\nTest Accuracy: 86.83%\nTest F1-Score: 74.73%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Bài 3","metadata":{}},{"cell_type":"markdown","source":"## Import module","metadata":{}},{"cell_type":"code","source":"from PhoNer import Vocab, PhoNER, collate_fn \nfrom seq2seq import Encoder, Decoder, Seq2Seq","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:29.629041Z","iopub.execute_input":"2025-11-27T10:42:29.629685Z","iopub.status.idle":"2025-11-27T10:42:29.636715Z","shell.execute_reply.started":"2025-11-27T10:42:29.629658Z","shell.execute_reply":"2025-11-27T10:42:29.636148Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"## Cấu hình","metadata":{}},{"cell_type":"code","source":"# --- CONFIG ---\nBATCH_SIZE = 32 \nENC_EMB_DIM = 128\nDEC_EMB_DIM = 128\nHIDDEN_DIM = 256 \nN_LAYERS = 5       \nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\nEPOCHS = 10\nLEARNING_RATE = 0.00001\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:29.638529Z","iopub.execute_input":"2025-11-27T10:42:29.638822Z","iopub.status.idle":"2025-11-27T10:42:29.654372Z","shell.execute_reply.started":"2025-11-27T10:42:29.638793Z","shell.execute_reply":"2025-11-27T10:42:29.653864Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"## Train, Eval bài 3","metadata":{}},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, tag_pad_idx):\n    model.eval()\n    epoch_loss = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            src = batch['input_ids'].to(DEVICE)\n            trg = batch['label'].to(DEVICE)\n\n            output = model(src, trg, 0) \n            \n            # output: [batch, seq_len, output_dim]\n            # trg: [batch, seq_len]\n            \n            # Reshape để tính loss\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim) # Bỏ token đầu\n            trg = trg[:, 1:].reshape(-1)                   # Bỏ token đầu\n            \n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            \n            preds = output.argmax(dim=1)\n            \n            preds_list = preds.cpu().numpy()\n            trg_list = trg.cpu().numpy()\n\n            valid_indices = np.where(trg_list != tag_pad_idx)[0]\n            \n            all_preds.extend(preds_list[valid_indices])\n            all_labels.extend(trg_list[valid_indices])\n            \n    avg_loss = epoch_loss / len(iterator)\n    f1 = f1_score(all_labels, all_preds, average='macro')\n    \n    return avg_loss, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:29.654987Z","iopub.execute_input":"2025-11-27T10:42:29.655192Z","iopub.status.idle":"2025-11-27T10:42:29.680464Z","shell.execute_reply.started":"2025-11-27T10:42:29.655165Z","shell.execute_reply":"2025-11-27T10:42:29.679960Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        src = batch['input_ids'].to(DEVICE)\n        trg = batch['label'].to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        # Reshape output và trg để tính CrossEntropy\n        \n        # output shape: [batch, seq_len, n_tags] -> flatten -> [batch * seq, n_tags]\n        # trg shape: [batch, seq_len] -> flatten -> [batch * seq]\n\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        \n        # tránh exploding gradient với LSTM nhiều lớp\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        if (i+1) % 10 == 0:\n             print(f\"Step {i+1}/{len(iterator)} | Loss: {loss.item():.4f}\")\n        \n    return epoch_loss / len(iterator)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:29.681140Z","iopub.execute_input":"2025-11-27T10:42:29.681323Z","iopub.status.idle":"2025-11-27T10:42:29.697272Z","shell.execute_reply.started":"2025-11-27T10:42:29.681302Z","shell.execute_reply":"2025-11-27T10:42:29.696771Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"## Main bài 3","metadata":{}},{"cell_type":"code","source":"def main():\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/PhoNER' \n    \n    print(\"Đang đọc dữ liệu...\")\n    vocab = Vocab(base_path)\n    print(f\"Vocab size: {vocab.len} | Tags: {vocab.n_labels}\")\n    \n    train_data = PhoNER(os.path.join(base_path, 'train_word.json'), vocab)\n    dev_data = PhoNER(os.path.join(base_path, 'dev_word.json'), vocab)\n    test_data = PhoNER(os.path.join(base_path, 'test_word.json'), vocab)\n    \n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    \n    # Model\n    input_dim = vocab.len\n    output_dim = vocab.n_labels\n    \n    enc = Encoder(input_dim, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n    dec = Decoder(output_dim, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n    \n    # Init weights (giúp hội tụ tốt hơn)\n    def init_weights(m):\n        for name, param in m.named_parameters():\n            nn.init.uniform_(param.data, -0.08, 0.08)\n    model.apply(init_weights)\n\n    print(f'Mô hình có {sum(p.numel() for p in model.parameters() if p.requires_grad):,} tham số train được.')\n\n    # 3. Optimizer & Loss\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # ignore_index=-100 để không tính loss cho phần padding\n    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    \n    # 4. Training \n    print(\"Bắt đầu huấn luyện...\")\n    best_valid_loss = float('inf')\n    \n    for epoch in range(EPOCHS):\n        train_loss = train(model, train_loader, optimizer, criterion, clip=1)\n        valid_loss, valid_f1 = evaluate(model, dev_loader, criterion, tag_pad_idx=-100)\n        \n        print(f'Epoch: {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_loss:.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1: {valid_f1*100:.2f}%')\n        \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'phoner_seq2seq.pt')\n    \n    # 5. Test\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    model.load_state_dict(torch.load('phoner_seq2seq.pt'))\n    test_loss, test_f1 = evaluate(model, test_loader, criterion, tag_pad_idx=-100)\n    print(f'Test Loss: {test_loss:.3f} | Test F1: {test_f1*100:.2f}%')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T10:42:29.697899Z","iopub.execute_input":"2025-11-27T10:42:29.698092Z","iopub.status.idle":"2025-11-27T10:47:28.947816Z","shell.execute_reply.started":"2025-11-27T10:42:29.698076Z","shell.execute_reply":"2025-11-27T10:47:28.947010Z"}},"outputs":[{"name":"stdout","text":"Đang đọc dữ liệu...\n--- Đang quét dữ liệu tại: /kaggle/working/DS201_Lab03-RNN/PhoNER ---\n-> Đã xây dựng Vocab: 7306 từ, 21 nhãn (bao gồm pad).\nVocab size: 7306 | Tags: 21\n-> Đã tải 5027 mẫu từ train_word.json\n-> Đã tải 2000 mẫu từ dev_word.json\n-> Đã tải 3000 mẫu từ test_word.json\nMô hình có 5,944,469 tham số train được.\nBắt đầu huấn luyện...\nStep 10/158 | Loss: 3.0565\nStep 20/158 | Loss: 3.0404\nStep 30/158 | Loss: 3.0316\nStep 40/158 | Loss: 3.0199\nStep 50/158 | Loss: 3.0102\nStep 60/158 | Loss: 2.9976\nStep 70/158 | Loss: 2.9809\nStep 80/158 | Loss: 2.9689\nStep 90/158 | Loss: 2.9561\nStep 100/158 | Loss: 2.9326\nStep 110/158 | Loss: 2.9091\nStep 120/158 | Loss: 2.8794\nStep 130/158 | Loss: 2.8510\nStep 140/158 | Loss: 2.8027\nStep 150/158 | Loss: 2.7039\nEpoch: 01\n\tTrain Loss: 2.941\n\t Val. Loss: 2.613 |  Val. F1: 5.55%\nStep 10/158 | Loss: 2.4800\nStep 20/158 | Loss: 2.2043\nStep 30/158 | Loss: 1.9478\nStep 40/158 | Loss: 1.6772\nStep 50/158 | Loss: 1.4575\nStep 60/158 | Loss: 1.4087\nStep 70/158 | Loss: 1.1313\nStep 80/158 | Loss: 1.2627\nStep 90/158 | Loss: 0.9374\nStep 100/158 | Loss: 1.1317\nStep 110/158 | Loss: 1.0211\nStep 120/158 | Loss: 1.1338\nStep 130/158 | Loss: 0.8918\nStep 140/158 | Loss: 0.7306\nStep 150/158 | Loss: 0.9149\nEpoch: 02\n\tTrain Loss: 1.423\n\t Val. Loss: 0.954 |  Val. F1: 7.62%\nStep 10/158 | Loss: 0.8343\nStep 20/158 | Loss: 0.7362\nStep 30/158 | Loss: 0.9395\nStep 40/158 | Loss: 1.0160\nStep 50/158 | Loss: 0.9891\nStep 60/158 | Loss: 0.8790\nStep 70/158 | Loss: 0.8797\nStep 80/158 | Loss: 0.8273\nStep 90/158 | Loss: 0.9000\nStep 100/158 | Loss: 0.8797\nStep 110/158 | Loss: 0.9399\nStep 120/158 | Loss: 0.9230\nStep 130/158 | Loss: 0.6043\nStep 140/158 | Loss: 0.8927\nStep 150/158 | Loss: 0.9061\nEpoch: 03\n\tTrain Loss: 0.854\n\t Val. Loss: 0.899 |  Val. F1: 7.75%\nStep 10/158 | Loss: 0.5315\nStep 20/158 | Loss: 0.7866\nStep 30/158 | Loss: 1.0457\nStep 40/158 | Loss: 0.6021\nStep 50/158 | Loss: 0.8381\nStep 60/158 | Loss: 0.9181\nStep 70/158 | Loss: 1.0027\nStep 80/158 | Loss: 0.9804\nStep 90/158 | Loss: 0.8453\nStep 100/158 | Loss: 1.0298\nStep 110/158 | Loss: 0.8816\nStep 120/158 | Loss: 0.7826\nStep 130/158 | Loss: 0.9709\nStep 140/158 | Loss: 1.0117\nStep 150/158 | Loss: 0.7749\nEpoch: 04\n\tTrain Loss: 0.812\n\t Val. Loss: 0.866 |  Val. F1: 7.84%\nStep 10/158 | Loss: 0.5932\nStep 20/158 | Loss: 0.8496\nStep 30/158 | Loss: 0.8128\nStep 40/158 | Loss: 0.8567\nStep 50/158 | Loss: 0.6219\nStep 60/158 | Loss: 0.7335\nStep 70/158 | Loss: 0.9969\nStep 80/158 | Loss: 0.8869\nStep 90/158 | Loss: 0.6706\nStep 100/158 | Loss: 0.8170\nStep 110/158 | Loss: 0.4297\nStep 120/158 | Loss: 0.7126\nStep 130/158 | Loss: 0.5742\nStep 140/158 | Loss: 0.7672\nStep 150/158 | Loss: 1.0244\nEpoch: 05\n\tTrain Loss: 0.750\n\t Val. Loss: 0.821 |  Val. F1: 8.08%\nStep 10/158 | Loss: 0.6762\nStep 20/158 | Loss: 0.6616\nStep 30/158 | Loss: 0.4904\nStep 40/158 | Loss: 0.9393\nStep 50/158 | Loss: 0.5499\nStep 60/158 | Loss: 0.8320\nStep 70/158 | Loss: 0.7531\nStep 80/158 | Loss: 0.6657\nStep 90/158 | Loss: 0.6403\nStep 100/158 | Loss: 0.6506\nStep 110/158 | Loss: 0.7918\nStep 120/158 | Loss: 0.6964\nStep 130/158 | Loss: 0.5142\nStep 140/158 | Loss: 0.6770\nStep 150/158 | Loss: 0.7195\nEpoch: 06\n\tTrain Loss: 0.712\n\t Val. Loss: 0.820 |  Val. F1: 8.11%\nStep 10/158 | Loss: 0.7553\nStep 20/158 | Loss: 0.8735\nStep 30/158 | Loss: 0.6744\nStep 40/158 | Loss: 0.6753\nStep 50/158 | Loss: 0.6937\nStep 60/158 | Loss: 0.7288\nStep 70/158 | Loss: 0.7246\nStep 80/158 | Loss: 0.6417\nStep 90/158 | Loss: 0.7439\nStep 100/158 | Loss: 0.7761\nStep 110/158 | Loss: 0.8046\nStep 120/158 | Loss: 0.5622\nStep 130/158 | Loss: 0.6273\nStep 140/158 | Loss: 0.7623\nStep 150/158 | Loss: 0.6315\nEpoch: 07\n\tTrain Loss: 0.691\n\t Val. Loss: 0.817 |  Val. F1: 8.11%\nStep 10/158 | Loss: 0.7723\nStep 20/158 | Loss: 0.6444\nStep 30/158 | Loss: 0.6301\nStep 40/158 | Loss: 0.5564\nStep 50/158 | Loss: 0.8491\nStep 60/158 | Loss: 0.7088\nStep 70/158 | Loss: 0.5871\nStep 80/158 | Loss: 0.6885\nStep 90/158 | Loss: 0.8578\nStep 100/158 | Loss: 0.8602\nStep 110/158 | Loss: 0.6112\nStep 120/158 | Loss: 0.5768\nStep 130/158 | Loss: 0.7791\nStep 140/158 | Loss: 0.7737\nStep 150/158 | Loss: 0.6333\nEpoch: 08\n\tTrain Loss: 0.703\n\t Val. Loss: 0.809 |  Val. F1: 8.12%\nStep 10/158 | Loss: 0.8800\nStep 20/158 | Loss: 0.6587\nStep 30/158 | Loss: 0.5958\nStep 40/158 | Loss: 0.6263\nStep 50/158 | Loss: 1.2030\nStep 60/158 | Loss: 0.6029\nStep 70/158 | Loss: 0.5907\nStep 80/158 | Loss: 1.0285\nStep 90/158 | Loss: 0.8169\nStep 100/158 | Loss: 0.8278\nStep 110/158 | Loss: 0.9353\nStep 120/158 | Loss: 0.7224\nStep 130/158 | Loss: 0.6291\nStep 140/158 | Loss: 0.7044\nStep 150/158 | Loss: 0.6873\nEpoch: 09\n\tTrain Loss: 0.714\n\t Val. Loss: 0.816 |  Val. F1: 8.06%\nStep 10/158 | Loss: 0.7943\nStep 20/158 | Loss: 0.7095\nStep 30/158 | Loss: 0.5624\nStep 40/158 | Loss: 0.5393\nStep 50/158 | Loss: 0.6409\nStep 60/158 | Loss: 0.6270\nStep 70/158 | Loss: 0.9785\nStep 80/158 | Loss: 0.6444\nStep 90/158 | Loss: 0.6518\nStep 100/158 | Loss: 0.6075\nStep 110/158 | Loss: 0.5600\nStep 120/158 | Loss: 0.4823\nStep 130/158 | Loss: 0.7309\nStep 140/158 | Loss: 0.8137\nStep 150/158 | Loss: 0.6594\nEpoch: 10\n\tTrain Loss: 0.701\n\t Val. Loss: 0.795 |  Val. F1: 8.21%\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.764 | Test F1: 7.92%\n","output_type":"stream"}],"execution_count":26}]}