{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Git","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/KhoiDang-KHDL/DS201_Lab03-RNN\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:56:58.774569Z","iopub.execute_input":"2025-11-25T02:56:58.774842Z","iopub.status.idle":"2025-11-25T02:56:59.628742Z","shell.execute_reply.started":"2025-11-25T02:56:58.774819Z","shell.execute_reply":"2025-11-25T02:56:59.628068Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'DS201_Lab03-RNN'...\nremote: Enumerating objects: 25, done.\u001b[K\nremote: Counting objects: 100% (25/25), done.\u001b[K\nremote: Compressing objects: 100% (18/18), done.\u001b[K\nremote: Total 25 (delta 10), reused 17 (delta 5), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (25/25), 1.09 MiB | 7.81 MiB/s, done.\nResolving deltas: 100% (10/10), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/DS201_Lab03-RNN') \n\n# Check thư mục\nimport os\nprint(os.listdir('/kaggle/working/DS201_Lab03-RNN'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:56:59.630417Z","iopub.execute_input":"2025-11-25T02:56:59.630659Z","iopub.status.idle":"2025-11-25T02:56:59.635496Z","shell.execute_reply.started":"2025-11-25T02:56:59.630636Z","shell.execute_reply":"2025-11-25T02:56:59.634937Z"}},"outputs":[{"name":"stdout","text":"['main.ipynb', 'PhoNer.py', 'lstm.py', 'uit_vsfc.py', '.git', 'seq2seq.py', 'UIT-VSFC-20251121T013658Z-1-001', 'PhoNER', 'gru.py']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Thư viện","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport os\nfrom sklearn.metrics import f1_score, accuracy_score\nimport numpy as np\nimport uit_vsfc\n\nfrom uit_vsfc import Vocab, UIT_VSFC, collate_fn\nfrom lstm import LSTMModel\nfrom gru import GRUModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:56:59.636136Z","iopub.execute_input":"2025-11-25T02:56:59.636304Z","iopub.status.idle":"2025-11-25T02:57:03.866646Z","shell.execute_reply.started":"2025-11-25T02:56:59.636290Z","shell.execute_reply":"2025-11-25T02:57:03.866085Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Cấu hình cho bài 1,2","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\nHIDDEN_SIZE = 128  \nN_LAYERS = 2\nLEARNING_RATE = 0.001\nEPOCHS = 10\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:57:03.867968Z","iopub.execute_input":"2025-11-25T02:57:03.868334Z","iopub.status.idle":"2025-11-25T02:57:03.950273Z","shell.execute_reply.started":"2025-11-25T02:57:03.868317Z","shell.execute_reply":"2025-11-25T02:57:03.949384Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Train, Eval bài 1,2","metadata":{}},{"cell_type":"code","source":"def evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n            \n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            \n            total_loss += loss.item()\n            \n            # Lấy nhãn dự đoán\n            _, predicted = torch.max(outputs, 1)\n            \n            # Chuyển về cpu để tính trong sklearn\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            \n    avg_loss = total_loss / len(dataloader)\n    \n    # Tính Accuracy và F1-Score\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, acc, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:57:03.951277Z","iopub.execute_input":"2025-11-25T02:57:03.951527Z","iopub.status.idle":"2025-11-25T02:57:03.965785Z","shell.execute_reply.started":"2025-11-25T02:57:03.951510Z","shell.execute_reply":"2025-11-25T02:57:03.965181Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train(model, train_loader, dev_loader, optimizer, criterion, epochs):\n    print(f\"Bắt đầu huấn luyện trên thiết bị: {DEVICE}\")\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        \n        for i, batch in enumerate(train_loader):\n            input_ids = batch['input_ids'].to(DEVICE)\n            labels = batch['label'].to(DEVICE)\n            \n            optimizer.zero_grad()\n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            \n            if (i + 1) % 10 == 0:\n                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n        dev_loss, dev_acc, dev_f1 = evaluate(model, dev_loader, criterion)\n        print(f\"--- Hết Epoch {epoch+1} ---\")\n        print(f\"Train Loss: {running_loss/len(train_loader):.4f}\")\n        print(f\"Dev Loss: {dev_loss:.4f} | Dev Acc: {dev_acc*100:.2f}% | Dev F1: {dev_f1*100:.2f}%\")\n        print(\"-\" * 30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:57:03.966575Z","iopub.execute_input":"2025-11-25T02:57:03.966848Z","iopub.status.idle":"2025-11-25T02:57:03.985657Z","shell.execute_reply.started":"2025-11-25T02:57:03.966822Z","shell.execute_reply":"2025-11-25T02:57:03.985038Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Bài 1: main","metadata":{}},{"cell_type":"code","source":"def main():\n\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/UIT-VSFC-20251121T013658Z-1-001/UIT-VSFC'\n    \n    train_path = os.path.join(base_path, 'UIT-VSFC-train.json')\n    dev_path = os.path.join(base_path, 'UIT-VSFC-dev.json')\n    test_path = os.path.join(base_path, 'UIT-VSFC-test.json')\n\n    print(\"Đang xây dựng bộ từ điển (Vocab):\")\n    vocab = Vocab(base_path)\n    print(f\"Kích thước từ điển: {vocab.len}\")\n\n    print(\"Đang tải dữ liệu...\")\n    train_dataset = UIT_VSFC(train_path, vocab)\n    dev_dataset = UIT_VSFC(dev_path, vocab)\n    test_dataset = UIT_VSFC(test_path, vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # --- MODEL ---\n    model = LSTMModel(\n        vocab_size=vocab.len,\n        hidden_size=HIDDEN_SIZE,\n        n_layers=N_LAYERS,\n        n_labels=vocab.n_labels,\n        padding_idx=vocab.w2i[vocab.pad]\n    ).to(DEVICE)\n\n    # ---  OPTIMIZER, LOSS ---\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train(model, train_loader, dev_loader, optimizer, criterion, EPOCHS)\n\n    # Evaluate\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"Test F1-Score: {test_f1*100:.2f}%\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:57:03.986618Z","iopub.execute_input":"2025-11-25T02:57:03.986871Z","iopub.status.idle":"2025-11-25T02:57:19.456098Z","shell.execute_reply.started":"2025-11-25T02:57:03.986849Z","shell.execute_reply":"2025-11-25T02:57:19.455427Z"}},"outputs":[{"name":"stdout","text":"Đang xây dựng bộ từ điển (Vocab)...\nKích thước từ điển: 2848\nĐang tải dữ liệu...\nBắt đầu huấn luyện trên thiết bị: cuda\nEpoch [1/10], Step [10/179], Loss: 1.1633\nEpoch [1/10], Step [20/179], Loss: 0.7219\nEpoch [1/10], Step [30/179], Loss: 0.9660\nEpoch [1/10], Step [40/179], Loss: 0.9687\nEpoch [1/10], Step [50/179], Loss: 0.8883\nEpoch [1/10], Step [60/179], Loss: 0.7369\nEpoch [1/10], Step [70/179], Loss: 0.7962\nEpoch [1/10], Step [80/179], Loss: 0.7773\nEpoch [1/10], Step [90/179], Loss: 0.8076\nEpoch [1/10], Step [100/179], Loss: 0.8188\nEpoch [1/10], Step [110/179], Loss: 0.8830\nEpoch [1/10], Step [120/179], Loss: 1.1580\nEpoch [1/10], Step [130/179], Loss: 0.8586\nEpoch [1/10], Step [140/179], Loss: 0.8490\nEpoch [1/10], Step [150/179], Loss: 0.8769\nEpoch [1/10], Step [160/179], Loss: 0.9175\nEpoch [1/10], Step [170/179], Loss: 0.9098\n--- Hết Epoch 1 ---\nTrain Loss: 0.8706\nDev Loss: 0.8377 | Dev Acc: 72.65% | Dev F1: 61.21%\n------------------------------\nEpoch [2/10], Step [10/179], Loss: 0.9899\nEpoch [2/10], Step [20/179], Loss: 0.7552\nEpoch [2/10], Step [30/179], Loss: 0.9481\nEpoch [2/10], Step [40/179], Loss: 1.0273\nEpoch [2/10], Step [50/179], Loss: 0.8154\nEpoch [2/10], Step [60/179], Loss: 0.8093\nEpoch [2/10], Step [70/179], Loss: 0.8846\nEpoch [2/10], Step [80/179], Loss: 0.5377\nEpoch [2/10], Step [90/179], Loss: 0.7533\nEpoch [2/10], Step [100/179], Loss: 0.7278\nEpoch [2/10], Step [110/179], Loss: 0.7454\nEpoch [2/10], Step [120/179], Loss: 0.7677\nEpoch [2/10], Step [130/179], Loss: 0.7734\nEpoch [2/10], Step [140/179], Loss: 0.7762\nEpoch [2/10], Step [150/179], Loss: 0.8484\nEpoch [2/10], Step [160/179], Loss: 0.7650\nEpoch [2/10], Step [170/179], Loss: 0.7036\n--- Hết Epoch 2 ---\nTrain Loss: 0.7987\nDev Loss: 0.7013 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [3/10], Step [10/179], Loss: 0.5503\nEpoch [3/10], Step [20/179], Loss: 0.5692\nEpoch [3/10], Step [30/179], Loss: 0.7769\nEpoch [3/10], Step [40/179], Loss: 0.5915\nEpoch [3/10], Step [50/179], Loss: 0.7867\nEpoch [3/10], Step [60/179], Loss: 0.7804\nEpoch [3/10], Step [70/179], Loss: 0.6765\nEpoch [3/10], Step [80/179], Loss: 0.8512\nEpoch [3/10], Step [90/179], Loss: 0.7620\nEpoch [3/10], Step [100/179], Loss: 0.5990\nEpoch [3/10], Step [110/179], Loss: 0.8313\nEpoch [3/10], Step [120/179], Loss: 0.6468\nEpoch [3/10], Step [130/179], Loss: 0.6414\nEpoch [3/10], Step [140/179], Loss: 0.7646\nEpoch [3/10], Step [150/179], Loss: 0.8642\nEpoch [3/10], Step [160/179], Loss: 0.5942\nEpoch [3/10], Step [170/179], Loss: 0.6655\n--- Hết Epoch 3 ---\nTrain Loss: 0.6988\nDev Loss: 0.6865 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [4/10], Step [10/179], Loss: 0.7736\nEpoch [4/10], Step [20/179], Loss: 0.4694\nEpoch [4/10], Step [30/179], Loss: 0.6751\nEpoch [4/10], Step [40/179], Loss: 0.6220\nEpoch [4/10], Step [50/179], Loss: 0.8248\nEpoch [4/10], Step [60/179], Loss: 0.6750\nEpoch [4/10], Step [70/179], Loss: 0.7856\nEpoch [4/10], Step [80/179], Loss: 0.5231\nEpoch [4/10], Step [90/179], Loss: 0.5136\nEpoch [4/10], Step [100/179], Loss: 0.5073\nEpoch [4/10], Step [110/179], Loss: 0.6403\nEpoch [4/10], Step [120/179], Loss: 0.4464\nEpoch [4/10], Step [130/179], Loss: 0.4448\nEpoch [4/10], Step [140/179], Loss: 0.7354\nEpoch [4/10], Step [150/179], Loss: 0.7432\nEpoch [4/10], Step [160/179], Loss: 0.5703\nEpoch [4/10], Step [170/179], Loss: 0.6037\n--- Hết Epoch 4 ---\nTrain Loss: 0.6653\nDev Loss: 0.6503 | Dev Acc: 69.36% | Dev F1: 69.30%\n------------------------------\nEpoch [5/10], Step [10/179], Loss: 0.7610\nEpoch [5/10], Step [20/179], Loss: 0.8499\nEpoch [5/10], Step [30/179], Loss: 0.6274\nEpoch [5/10], Step [40/179], Loss: 0.7528\nEpoch [5/10], Step [50/179], Loss: 0.6440\nEpoch [5/10], Step [60/179], Loss: 0.5884\nEpoch [5/10], Step [70/179], Loss: 0.6989\nEpoch [5/10], Step [80/179], Loss: 0.6317\nEpoch [5/10], Step [90/179], Loss: 0.4811\nEpoch [5/10], Step [100/179], Loss: 0.5512\nEpoch [5/10], Step [110/179], Loss: 0.6076\nEpoch [5/10], Step [120/179], Loss: 0.6584\nEpoch [5/10], Step [130/179], Loss: 0.7898\nEpoch [5/10], Step [140/179], Loss: 0.5429\nEpoch [5/10], Step [150/179], Loss: 0.6976\nEpoch [5/10], Step [160/179], Loss: 0.5864\nEpoch [5/10], Step [170/179], Loss: 0.6141\n--- Hết Epoch 5 ---\nTrain Loss: 0.6590\nDev Loss: 0.6513 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [6/10], Step [10/179], Loss: 0.7196\nEpoch [6/10], Step [20/179], Loss: 0.8903\nEpoch [6/10], Step [30/179], Loss: 0.5018\nEpoch [6/10], Step [40/179], Loss: 0.5562\nEpoch [6/10], Step [50/179], Loss: 0.9219\nEpoch [6/10], Step [60/179], Loss: 0.5384\nEpoch [6/10], Step [70/179], Loss: 0.5945\nEpoch [6/10], Step [80/179], Loss: 0.7188\nEpoch [6/10], Step [90/179], Loss: 0.5758\nEpoch [6/10], Step [100/179], Loss: 0.6106\nEpoch [6/10], Step [110/179], Loss: 0.5261\nEpoch [6/10], Step [120/179], Loss: 0.5773\nEpoch [6/10], Step [130/179], Loss: 0.7108\nEpoch [6/10], Step [140/179], Loss: 0.6244\nEpoch [6/10], Step [150/179], Loss: 0.9314\nEpoch [6/10], Step [160/179], Loss: 0.7738\nEpoch [6/10], Step [170/179], Loss: 0.6570\n--- Hết Epoch 6 ---\nTrain Loss: 0.6598\nDev Loss: 0.6514 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [7/10], Step [10/179], Loss: 0.4795\nEpoch [7/10], Step [20/179], Loss: 0.7130\nEpoch [7/10], Step [30/179], Loss: 0.5418\nEpoch [7/10], Step [40/179], Loss: 0.5941\nEpoch [7/10], Step [50/179], Loss: 0.7376\nEpoch [7/10], Step [60/179], Loss: 0.7019\nEpoch [7/10], Step [70/179], Loss: 0.7610\nEpoch [7/10], Step [80/179], Loss: 0.6798\nEpoch [7/10], Step [90/179], Loss: 0.5773\nEpoch [7/10], Step [100/179], Loss: 0.7603\nEpoch [7/10], Step [110/179], Loss: 0.5314\nEpoch [7/10], Step [120/179], Loss: 0.5745\nEpoch [7/10], Step [130/179], Loss: 0.5612\nEpoch [7/10], Step [140/179], Loss: 0.5719\nEpoch [7/10], Step [150/179], Loss: 0.5292\nEpoch [7/10], Step [160/179], Loss: 0.7731\nEpoch [7/10], Step [170/179], Loss: 0.4669\n--- Hết Epoch 7 ---\nTrain Loss: 0.6574\nDev Loss: 0.6488 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [8/10], Step [10/179], Loss: 0.5519\nEpoch [8/10], Step [20/179], Loss: 0.6938\nEpoch [8/10], Step [30/179], Loss: 0.5972\nEpoch [8/10], Step [40/179], Loss: 0.5872\nEpoch [8/10], Step [50/179], Loss: 0.5797\nEpoch [8/10], Step [60/179], Loss: 0.5348\nEpoch [8/10], Step [70/179], Loss: 0.8041\nEpoch [8/10], Step [80/179], Loss: 0.6392\nEpoch [8/10], Step [90/179], Loss: 0.7651\nEpoch [8/10], Step [100/179], Loss: 0.8271\nEpoch [8/10], Step [110/179], Loss: 0.6933\nEpoch [8/10], Step [120/179], Loss: 0.7842\nEpoch [8/10], Step [130/179], Loss: 0.7266\nEpoch [8/10], Step [140/179], Loss: 0.8039\nEpoch [8/10], Step [150/179], Loss: 0.9070\nEpoch [8/10], Step [160/179], Loss: 0.7790\nEpoch [8/10], Step [170/179], Loss: 0.6014\n--- Hết Epoch 8 ---\nTrain Loss: 0.7418\nDev Loss: 0.7998 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [9/10], Step [10/179], Loss: 0.7068\nEpoch [9/10], Step [20/179], Loss: 0.8696\nEpoch [9/10], Step [30/179], Loss: 0.5756\nEpoch [9/10], Step [40/179], Loss: 0.7372\nEpoch [9/10], Step [50/179], Loss: 0.8026\nEpoch [9/10], Step [60/179], Loss: 0.8288\nEpoch [9/10], Step [70/179], Loss: 0.9150\nEpoch [9/10], Step [80/179], Loss: 0.8797\nEpoch [9/10], Step [90/179], Loss: 0.7431\nEpoch [9/10], Step [100/179], Loss: 0.9118\nEpoch [9/10], Step [110/179], Loss: 0.8328\nEpoch [9/10], Step [120/179], Loss: 1.0606\nEpoch [9/10], Step [130/179], Loss: 0.8528\nEpoch [9/10], Step [140/179], Loss: 0.6908\nEpoch [9/10], Step [150/179], Loss: 0.8103\nEpoch [9/10], Step [160/179], Loss: 0.7658\nEpoch [9/10], Step [170/179], Loss: 0.7390\n--- Hết Epoch 9 ---\nTrain Loss: 0.8014\nDev Loss: 0.7967 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\nEpoch [10/10], Step [10/179], Loss: 0.8832\nEpoch [10/10], Step [20/179], Loss: 0.6303\nEpoch [10/10], Step [30/179], Loss: 0.8565\nEpoch [10/10], Step [40/179], Loss: 0.8076\nEpoch [10/10], Step [50/179], Loss: 0.9307\nEpoch [10/10], Step [60/179], Loss: 0.7970\nEpoch [10/10], Step [70/179], Loss: 0.8618\nEpoch [10/10], Step [80/179], Loss: 0.9079\nEpoch [10/10], Step [90/179], Loss: 0.6607\nEpoch [10/10], Step [100/179], Loss: 0.8532\nEpoch [10/10], Step [110/179], Loss: 0.9110\nEpoch [10/10], Step [120/179], Loss: 0.7847\nEpoch [10/10], Step [130/179], Loss: 0.7553\nEpoch [10/10], Step [140/179], Loss: 0.6986\nEpoch [10/10], Step [150/179], Loss: 0.7439\nEpoch [10/10], Step [160/179], Loss: 0.9757\nEpoch [10/10], Step [170/179], Loss: 0.6129\n--- Hết Epoch 10 ---\nTrain Loss: 0.8016\nDev Loss: 0.7956 | Dev Acc: 72.71% | Dev F1: 61.22%\n------------------------------\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.7937\nTest Accuracy: 72.33%\nTest F1-Score: 60.72%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Bài 2: main","metadata":{}},{"cell_type":"code","source":"def main():\n\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/UIT-VSFC-20251121T013658Z-1-001/UIT-VSFC'\n    \n    train_path = os.path.join(base_path, 'UIT-VSFC-train.json')\n    dev_path = os.path.join(base_path, 'UIT-VSFC-dev.json')\n    test_path = os.path.join(base_path, 'UIT-VSFC-test.json')\n\n    print(\"Đang xây dựng bộ từ điển (Vocab)...\")\n    vocab = Vocab(base_path)\n    print(f\"Kích thước từ điển: {vocab.len}\")\n\n    print(\"Đang tải dữ liệu...\")\n    train_dataset = UIT_VSFC(train_path, vocab)\n    dev_dataset = UIT_VSFC(dev_path, vocab)\n    test_dataset = UIT_VSFC(test_path, vocab)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n    # ---  MODEL ---\n    model = GRUModel(\n        vocab_size=vocab.len,\n        embedding_dim=128,\n        hidden_size=HIDDEN_SIZE,\n        n_layers=N_LAYERS,\n        n_labels=vocab.n_labels,\n        padding_idx=vocab.w2i[vocab.pad]\n    ).to(DEVICE)\n\n    # ---  OPTIMIZER, LOSS ---\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train(model, train_loader, dev_loader, optimizer, criterion, EPOCHS)\n    \n    # Evaluate\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    test_loss, test_acc, test_f1 = evaluate(model, test_loader, criterion)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"Test F1-Score: {test_f1*100:.2f}%\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:57:19.456931Z","iopub.execute_input":"2025-11-25T02:57:19.457361Z","iopub.status.idle":"2025-11-25T02:57:34.233769Z","shell.execute_reply.started":"2025-11-25T02:57:19.457332Z","shell.execute_reply":"2025-11-25T02:57:34.233149Z"}},"outputs":[{"name":"stdout","text":"Đang xây dựng bộ từ điển (Vocab)...\nKích thước từ điển: 2848\nĐang tải dữ liệu...\nBắt đầu huấn luyện trên thiết bị: cuda\nEpoch [1/10], Step [10/179], Loss: 0.9440\nEpoch [1/10], Step [20/179], Loss: 0.7779\nEpoch [1/10], Step [30/179], Loss: 0.5210\nEpoch [1/10], Step [40/179], Loss: 0.4982\nEpoch [1/10], Step [50/179], Loss: 0.4534\nEpoch [1/10], Step [60/179], Loss: 0.7262\nEpoch [1/10], Step [70/179], Loss: 0.3585\nEpoch [1/10], Step [80/179], Loss: 0.4916\nEpoch [1/10], Step [90/179], Loss: 0.4982\nEpoch [1/10], Step [100/179], Loss: 0.4240\nEpoch [1/10], Step [110/179], Loss: 0.3566\nEpoch [1/10], Step [120/179], Loss: 0.4209\nEpoch [1/10], Step [130/179], Loss: 0.2956\nEpoch [1/10], Step [140/179], Loss: 0.2559\nEpoch [1/10], Step [150/179], Loss: 0.2795\nEpoch [1/10], Step [160/179], Loss: 0.4488\nEpoch [1/10], Step [170/179], Loss: 0.3230\n--- Hết Epoch 1 ---\nTrain Loss: 0.4708\nDev Loss: 0.3684 | Dev Acc: 86.10% | Dev F1: 85.99%\n------------------------------\nEpoch [2/10], Step [10/179], Loss: 0.2791\nEpoch [2/10], Step [20/179], Loss: 0.4638\nEpoch [2/10], Step [30/179], Loss: 0.5126\nEpoch [2/10], Step [40/179], Loss: 0.3579\nEpoch [2/10], Step [50/179], Loss: 0.3202\nEpoch [2/10], Step [60/179], Loss: 0.4282\nEpoch [2/10], Step [70/179], Loss: 0.2489\nEpoch [2/10], Step [80/179], Loss: 0.4036\nEpoch [2/10], Step [90/179], Loss: 0.4763\nEpoch [2/10], Step [100/179], Loss: 0.3375\nEpoch [2/10], Step [110/179], Loss: 0.2881\nEpoch [2/10], Step [120/179], Loss: 0.2378\nEpoch [2/10], Step [130/179], Loss: 0.3405\nEpoch [2/10], Step [140/179], Loss: 0.3222\nEpoch [2/10], Step [150/179], Loss: 0.2063\nEpoch [2/10], Step [160/179], Loss: 0.3579\nEpoch [2/10], Step [170/179], Loss: 0.2082\n--- Hết Epoch 2 ---\nTrain Loss: 0.3277\nDev Loss: 0.3518 | Dev Acc: 87.30% | Dev F1: 87.56%\n------------------------------\nEpoch [3/10], Step [10/179], Loss: 0.2304\nEpoch [3/10], Step [20/179], Loss: 0.2054\nEpoch [3/10], Step [30/179], Loss: 0.3164\nEpoch [3/10], Step [40/179], Loss: 0.1619\nEpoch [3/10], Step [50/179], Loss: 0.1480\nEpoch [3/10], Step [60/179], Loss: 0.2341\nEpoch [3/10], Step [70/179], Loss: 0.1419\nEpoch [3/10], Step [80/179], Loss: 0.1806\nEpoch [3/10], Step [90/179], Loss: 0.2892\nEpoch [3/10], Step [100/179], Loss: 0.4196\nEpoch [3/10], Step [110/179], Loss: 0.3016\nEpoch [3/10], Step [120/179], Loss: 0.2185\nEpoch [3/10], Step [130/179], Loss: 0.4208\nEpoch [3/10], Step [140/179], Loss: 0.1628\nEpoch [3/10], Step [150/179], Loss: 0.2561\nEpoch [3/10], Step [160/179], Loss: 0.3156\nEpoch [3/10], Step [170/179], Loss: 0.3260\n--- Hết Epoch 3 ---\nTrain Loss: 0.2739\nDev Loss: 0.3645 | Dev Acc: 87.30% | Dev F1: 86.98%\n------------------------------\nEpoch [4/10], Step [10/179], Loss: 0.0881\nEpoch [4/10], Step [20/179], Loss: 0.2832\nEpoch [4/10], Step [30/179], Loss: 0.1367\nEpoch [4/10], Step [40/179], Loss: 0.3726\nEpoch [4/10], Step [50/179], Loss: 0.2036\nEpoch [4/10], Step [60/179], Loss: 0.2045\nEpoch [4/10], Step [70/179], Loss: 0.2766\nEpoch [4/10], Step [80/179], Loss: 0.2542\nEpoch [4/10], Step [90/179], Loss: 0.2116\nEpoch [4/10], Step [100/179], Loss: 0.1659\nEpoch [4/10], Step [110/179], Loss: 0.1679\nEpoch [4/10], Step [120/179], Loss: 0.1628\nEpoch [4/10], Step [130/179], Loss: 0.1695\nEpoch [4/10], Step [140/179], Loss: 0.3667\nEpoch [4/10], Step [150/179], Loss: 0.3087\nEpoch [4/10], Step [160/179], Loss: 0.1213\nEpoch [4/10], Step [170/179], Loss: 0.3294\n--- Hết Epoch 4 ---\nTrain Loss: 0.2319\nDev Loss: 0.3779 | Dev Acc: 87.49% | Dev F1: 87.36%\n------------------------------\nEpoch [5/10], Step [10/179], Loss: 0.2349\nEpoch [5/10], Step [20/179], Loss: 0.2454\nEpoch [5/10], Step [30/179], Loss: 0.1374\nEpoch [5/10], Step [40/179], Loss: 0.2451\nEpoch [5/10], Step [50/179], Loss: 0.1283\nEpoch [5/10], Step [60/179], Loss: 0.0980\nEpoch [5/10], Step [70/179], Loss: 0.1074\nEpoch [5/10], Step [80/179], Loss: 0.1193\nEpoch [5/10], Step [90/179], Loss: 0.1400\nEpoch [5/10], Step [100/179], Loss: 0.0901\nEpoch [5/10], Step [110/179], Loss: 0.1286\nEpoch [5/10], Step [120/179], Loss: 0.1169\nEpoch [5/10], Step [130/179], Loss: 0.0937\nEpoch [5/10], Step [140/179], Loss: 0.1625\nEpoch [5/10], Step [150/179], Loss: 0.1961\nEpoch [5/10], Step [160/179], Loss: 0.1824\nEpoch [5/10], Step [170/179], Loss: 0.1336\n--- Hết Epoch 5 ---\nTrain Loss: 0.1897\nDev Loss: 0.3938 | Dev Acc: 86.86% | Dev F1: 86.77%\n------------------------------\nEpoch [6/10], Step [10/179], Loss: 0.1395\nEpoch [6/10], Step [20/179], Loss: 0.2020\nEpoch [6/10], Step [30/179], Loss: 0.2348\nEpoch [6/10], Step [40/179], Loss: 0.0801\nEpoch [6/10], Step [50/179], Loss: 0.0686\nEpoch [6/10], Step [60/179], Loss: 0.0503\nEpoch [6/10], Step [70/179], Loss: 0.2078\nEpoch [6/10], Step [80/179], Loss: 0.2665\nEpoch [6/10], Step [90/179], Loss: 0.0714\nEpoch [6/10], Step [100/179], Loss: 0.5518\nEpoch [6/10], Step [110/179], Loss: 0.1419\nEpoch [6/10], Step [120/179], Loss: 0.0736\nEpoch [6/10], Step [130/179], Loss: 0.1236\nEpoch [6/10], Step [140/179], Loss: 0.1328\nEpoch [6/10], Step [150/179], Loss: 0.1461\nEpoch [6/10], Step [160/179], Loss: 0.2467\nEpoch [6/10], Step [170/179], Loss: 0.1436\n--- Hết Epoch 6 ---\nTrain Loss: 0.1525\nDev Loss: 0.4216 | Dev Acc: 86.92% | Dev F1: 86.61%\n------------------------------\nEpoch [7/10], Step [10/179], Loss: 0.0784\nEpoch [7/10], Step [20/179], Loss: 0.1053\nEpoch [7/10], Step [30/179], Loss: 0.0378\nEpoch [7/10], Step [40/179], Loss: 0.1123\nEpoch [7/10], Step [50/179], Loss: 0.0371\nEpoch [7/10], Step [60/179], Loss: 0.0607\nEpoch [7/10], Step [70/179], Loss: 0.0639\nEpoch [7/10], Step [80/179], Loss: 0.1555\nEpoch [7/10], Step [90/179], Loss: 0.0595\nEpoch [7/10], Step [100/179], Loss: 0.0390\nEpoch [7/10], Step [110/179], Loss: 0.1207\nEpoch [7/10], Step [120/179], Loss: 0.0738\nEpoch [7/10], Step [130/179], Loss: 0.0472\nEpoch [7/10], Step [140/179], Loss: 0.1344\nEpoch [7/10], Step [150/179], Loss: 0.1134\nEpoch [7/10], Step [160/179], Loss: 0.0701\nEpoch [7/10], Step [170/179], Loss: 0.1522\n--- Hết Epoch 7 ---\nTrain Loss: 0.1172\nDev Loss: 0.4717 | Dev Acc: 86.73% | Dev F1: 86.69%\n------------------------------\nEpoch [8/10], Step [10/179], Loss: 0.0426\nEpoch [8/10], Step [20/179], Loss: 0.1028\nEpoch [8/10], Step [30/179], Loss: 0.1070\nEpoch [8/10], Step [40/179], Loss: 0.1486\nEpoch [8/10], Step [50/179], Loss: 0.0554\nEpoch [8/10], Step [60/179], Loss: 0.0955\nEpoch [8/10], Step [70/179], Loss: 0.1147\nEpoch [8/10], Step [80/179], Loss: 0.0347\nEpoch [8/10], Step [90/179], Loss: 0.0621\nEpoch [8/10], Step [100/179], Loss: 0.0438\nEpoch [8/10], Step [110/179], Loss: 0.0428\nEpoch [8/10], Step [120/179], Loss: 0.0351\nEpoch [8/10], Step [130/179], Loss: 0.2425\nEpoch [8/10], Step [140/179], Loss: 0.0485\nEpoch [8/10], Step [150/179], Loss: 0.0340\nEpoch [8/10], Step [160/179], Loss: 0.1177\nEpoch [8/10], Step [170/179], Loss: 0.0347\n--- Hết Epoch 8 ---\nTrain Loss: 0.0945\nDev Loss: 0.5146 | Dev Acc: 86.73% | Dev F1: 86.64%\n------------------------------\nEpoch [9/10], Step [10/179], Loss: 0.0509\nEpoch [9/10], Step [20/179], Loss: 0.1081\nEpoch [9/10], Step [30/179], Loss: 0.0261\nEpoch [9/10], Step [40/179], Loss: 0.0403\nEpoch [9/10], Step [50/179], Loss: 0.0908\nEpoch [9/10], Step [60/179], Loss: 0.0880\nEpoch [9/10], Step [70/179], Loss: 0.1652\nEpoch [9/10], Step [80/179], Loss: 0.0627\nEpoch [9/10], Step [90/179], Loss: 0.0733\nEpoch [9/10], Step [100/179], Loss: 0.0638\nEpoch [9/10], Step [110/179], Loss: 0.0983\nEpoch [9/10], Step [120/179], Loss: 0.0171\nEpoch [9/10], Step [130/179], Loss: 0.1799\nEpoch [9/10], Step [140/179], Loss: 0.1357\nEpoch [9/10], Step [150/179], Loss: 0.0782\nEpoch [9/10], Step [160/179], Loss: 0.1627\nEpoch [9/10], Step [170/179], Loss: 0.0760\n--- Hết Epoch 9 ---\nTrain Loss: 0.0644\nDev Loss: 0.5652 | Dev Acc: 86.92% | Dev F1: 87.06%\n------------------------------\nEpoch [10/10], Step [10/179], Loss: 0.0399\nEpoch [10/10], Step [20/179], Loss: 0.0862\nEpoch [10/10], Step [30/179], Loss: 0.0048\nEpoch [10/10], Step [40/179], Loss: 0.0862\nEpoch [10/10], Step [50/179], Loss: 0.0411\nEpoch [10/10], Step [60/179], Loss: 0.0653\nEpoch [10/10], Step [70/179], Loss: 0.1221\nEpoch [10/10], Step [80/179], Loss: 0.0648\nEpoch [10/10], Step [90/179], Loss: 0.0063\nEpoch [10/10], Step [100/179], Loss: 0.0264\nEpoch [10/10], Step [110/179], Loss: 0.0172\nEpoch [10/10], Step [120/179], Loss: 0.0150\nEpoch [10/10], Step [130/179], Loss: 0.1066\nEpoch [10/10], Step [140/179], Loss: 0.1484\nEpoch [10/10], Step [150/179], Loss: 0.0848\nEpoch [10/10], Step [160/179], Loss: 0.0831\nEpoch [10/10], Step [170/179], Loss: 0.0152\n--- Hết Epoch 10 ---\nTrain Loss: 0.0535\nDev Loss: 0.5774 | Dev Acc: 87.24% | Dev F1: 87.03%\n------------------------------\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.5862\nTest Accuracy: 86.54%\nTest F1-Score: 86.49%\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Bài 3","metadata":{}},{"cell_type":"markdown","source":"## Import module","metadata":{}},{"cell_type":"code","source":"from PhoNer import Vocab, PhoNER, collate_fn \nfrom seq2seq import Encoder, Decoder, Seq2Seq","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cấu hình","metadata":{}},{"cell_type":"code","source":"# --- CONFIG ---\nBATCH_SIZE = 32 \nENC_EMB_DIM = 100\nDEC_EMB_DIM = 100\nHIDDEN_DIM = 256 \nN_LAYERS = 5       \nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\nEPOCHS = 10\nLEARNING_RATE = 0.001\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train, Eval bài 3","metadata":{}},{"cell_type":"code","source":"def evaluate(model, iterator, criterion, tag_pad_idx):\n    model.eval()\n    epoch_loss = 0\n    \n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in iterator:\n            src = batch['input_ids'].to(DEVICE)\n            trg = batch['label'].to(DEVICE)\n\n            # Tắt teacher forcing khi evaluate (ratio = 0)\n            output = model(src, trg, 0) \n            \n            # output: [batch, seq_len, output_dim]\n            # trg: [batch, seq_len]\n            \n            # Reshape để tính loss\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim) # Bỏ token đầu\n            trg = trg[:, 1:].reshape(-1)                   # Bỏ token đầu\n            \n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n            \n            # --- TÍNH F1 ---\n            # Lấy nhãn dự đoán\n            preds = output.argmax(dim=1)\n            \n            # Chuyển về CPU list\n            preds_list = preds.cpu().numpy()\n            trg_list = trg.cpu().numpy()\n            \n            # Lọc bỏ các giá trị Padding trong trg để tính F1 chính xác\n            # Vì trg có pad_idx là -100\n            valid_indices = np.where(trg_list != tag_pad_idx)[0]\n            \n            all_preds.extend(preds_list[valid_indices])\n            all_labels.extend(trg_list[valid_indices])\n            \n    avg_loss = epoch_loss / len(iterator)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    \n    return avg_loss, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in enumerate(iterator):\n        src = batch['input_ids'].to(DEVICE)\n        trg = batch['label'].to(DEVICE)\n        \n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        # Reshape output và trg để tính CrossEntropy\n        # output shape: [batch, seq_len, n_tags] -> flatten -> [batch * seq, n_tags]\n        # trg shape: [batch, seq_len] -> flatten -> [batch * seq]\n        \n        # Lưu ý: Bỏ qua phần tử đầu tiên (index 0) vì Seq2Seq start loop từ 1\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        \n        # Cắt gradient để tránh bùng nổ gradient (exploding gradient) với LSTM nhiều lớp\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n        \n        if (i+1) % 10 == 0:\n             print(f\"Step {i+1}/{len(iterator)} | Loss: {loss.item():.4f}\")\n        \n    return epoch_loss / len(iterator)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Main bài 3","metadata":{}},{"cell_type":"code","source":"def main():\n    # 1. Load Data\n    # Sửa đường dẫn tới thư mục chứa file json của PhoNER\n    base_path = r'/kaggle/working/DS201_Lab03-RNN/PhoNER' \n    \n    print(\"Đang đọc dữ liệu...\")\n    vocab = Vocab(base_path)\n    print(f\"Vocab size: {vocab.len} | Tags: {vocab.n_labels}\")\n    \n    train_data = PhoNER(os.path.join(base_path, 'train_word.json'), vocab)\n    dev_data = PhoNER(os.path.join(base_path, 'dev_word.json'), vocab)\n    test_data = PhoNER(os.path.join(base_path, 'test_word.json'), vocab)\n    \n    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n    dev_loader = DataLoader(dev_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n    \n    # 2. Khởi tạo Model\n    input_dim = vocab.len\n    output_dim = vocab.n_labels\n    \n    enc = Encoder(input_dim, ENC_EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n    dec = Decoder(output_dim, DEC_EMB_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n    model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n    \n    # Init weights (tùy chọn, giúp hội tụ tốt hơn)\n    def init_weights(m):\n        for name, param in m.named_parameters():\n            nn.init.uniform_(param.data, -0.08, 0.08)\n    model.apply(init_weights)\n\n    print(f'Mô hình có {sum(p.numel() for p in model.parameters() if p.requires_grad):,} tham số train được.')\n\n    # 3. Optimizer & Loss\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # ignore_index=-100 để không tính loss cho phần padding\n    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n    \n    # 4. Training Loop\n    print(\"Bắt đầu huấn luyện...\")\n    best_valid_loss = float('inf')\n    \n    for epoch in range(EPOCHS):\n        train_loss = train(model, train_loader, optimizer, criterion, clip=1)\n        valid_loss, valid_f1 = evaluate(model, dev_loader, criterion, tag_pad_idx=-100)\n        \n        print(f'Epoch: {epoch+1:02}')\n        print(f'\\tTrain Loss: {train_loss:.3f}')\n        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. F1: {valid_f1*100:.2f}%')\n        \n        if valid_loss < best_valid_loss:\n            best_valid_loss = valid_loss\n            torch.save(model.state_dict(), 'phoner_seq2seq.pt')\n    \n    # 5. Test\n    print(\"\\nĐang đánh giá trên tập Test...\")\n    model.load_state_dict(torch.load('phoner_seq2seq.pt'))\n    test_loss, test_f1 = evaluate(model, test_loader, criterion, tag_pad_idx=-100)\n    print(f'Test Loss: {test_loss:.3f} | Test F1: {test_f1*100:.2f}%')\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-25T02:57:34.234551Z","iopub.execute_input":"2025-11-25T02:57:34.234815Z","iopub.status.idle":"2025-11-25T03:02:32.093450Z","shell.execute_reply.started":"2025-11-25T02:57:34.234795Z","shell.execute_reply":"2025-11-25T03:02:32.092608Z"}},"outputs":[{"name":"stdout","text":"Đang đọc dữ liệu...\n--- Đang quét dữ liệu tại: /kaggle/working/DS201_Lab03-RNN/PhoNER ---\n-> Đã xây dựng Vocab: 7306 từ, 21 nhãn (bao gồm pad).\nVocab size: 7306 | Tags: 21\n-> Đã tải 5027 mẫu từ train_word.json\n-> Đã tải 2000 mẫu từ dev_word.json\n-> Đã tải 3000 mẫu từ test_word.json\nMô hình có 5,681,969 tham số train được.\nBắt đầu huấn luyện...\nStep 10/158 | Loss: 0.8964\nStep 20/158 | Loss: 0.7721\nStep 30/158 | Loss: 0.8356\nStep 40/158 | Loss: 0.8225\nStep 50/158 | Loss: 0.6643\nStep 60/158 | Loss: 0.7339\nStep 70/158 | Loss: 0.7028\nStep 80/158 | Loss: 0.6225\nStep 90/158 | Loss: 0.7005\nStep 100/158 | Loss: 0.7773\nStep 110/158 | Loss: 0.6410\nStep 120/158 | Loss: 0.6180\nStep 130/158 | Loss: 0.4686\nStep 140/158 | Loss: 0.6367\nStep 150/158 | Loss: 0.5616\nEpoch: 01\n\tTrain Loss: 0.745\n\t Val. Loss: 1.451 |  Val. F1: 15.11%\nStep 10/158 | Loss: 0.6464\nStep 20/158 | Loss: 0.5114\nStep 30/158 | Loss: 0.3016\nStep 40/158 | Loss: 0.4225\nStep 50/158 | Loss: 0.6511\nStep 60/158 | Loss: 0.7583\nStep 70/158 | Loss: 0.5766\nStep 80/158 | Loss: 0.5955\nStep 90/158 | Loss: 0.7384\nStep 100/158 | Loss: 0.6010\nStep 110/158 | Loss: 0.7877\nStep 120/158 | Loss: 0.7099\nStep 130/158 | Loss: 0.5152\nStep 140/158 | Loss: 0.4394\nStep 150/158 | Loss: 0.6266\nEpoch: 02\n\tTrain Loss: 0.545\n\t Val. Loss: 1.535 |  Val. F1: 15.41%\nStep 10/158 | Loss: 0.6221\nStep 20/158 | Loss: 0.3633\nStep 30/158 | Loss: 0.5101\nStep 40/158 | Loss: 0.3718\nStep 50/158 | Loss: 0.5438\nStep 60/158 | Loss: 0.4351\nStep 70/158 | Loss: 0.4511\nStep 80/158 | Loss: 0.6490\nStep 90/158 | Loss: 0.4830\nStep 100/158 | Loss: 0.5505\nStep 110/158 | Loss: 0.6853\nStep 120/158 | Loss: 0.5394\nStep 130/158 | Loss: 0.5598\nStep 140/158 | Loss: 0.3703\nStep 150/158 | Loss: 0.5392\nEpoch: 03\n\tTrain Loss: 0.535\n\t Val. Loss: 1.604 |  Val. F1: 15.11%\nStep 10/158 | Loss: 0.5459\nStep 20/158 | Loss: 0.4880\nStep 30/158 | Loss: 0.4005\nStep 40/158 | Loss: 0.4949\nStep 50/158 | Loss: 0.5784\nStep 60/158 | Loss: 0.4420\nStep 70/158 | Loss: 0.5649\nStep 80/158 | Loss: 0.4598\nStep 90/158 | Loss: 0.4998\nStep 100/158 | Loss: 0.4521\nStep 110/158 | Loss: 0.5451\nStep 120/158 | Loss: 0.4225\nStep 130/158 | Loss: 0.6589\nStep 140/158 | Loss: 0.5087\nStep 150/158 | Loss: 0.5511\nEpoch: 04\n\tTrain Loss: 0.527\n\t Val. Loss: 1.677 |  Val. F1: 15.11%\nStep 10/158 | Loss: 0.5075\nStep 20/158 | Loss: 0.4723\nStep 30/158 | Loss: 0.5673\nStep 40/158 | Loss: 0.5826\nStep 50/158 | Loss: 0.5220\nStep 60/158 | Loss: 0.5047\nStep 70/158 | Loss: 0.4814\nStep 80/158 | Loss: 0.5640\nStep 90/158 | Loss: 0.4775\nStep 100/158 | Loss: 0.6758\nStep 110/158 | Loss: 0.5379\nStep 120/158 | Loss: 0.4255\nStep 130/158 | Loss: 0.4606\nStep 140/158 | Loss: 0.4890\nStep 150/158 | Loss: 0.5256\nEpoch: 05\n\tTrain Loss: 0.500\n\t Val. Loss: 1.407 |  Val. F1: 15.47%\nStep 10/158 | Loss: 0.4622\nStep 20/158 | Loss: 0.4828\nStep 30/158 | Loss: 0.3601\nStep 40/158 | Loss: 0.4302\nStep 50/158 | Loss: 0.4656\nStep 60/158 | Loss: 0.2825\nStep 70/158 | Loss: 0.3904\nStep 80/158 | Loss: 0.3436\nStep 90/158 | Loss: 0.4379\nStep 100/158 | Loss: 0.5703\nStep 110/158 | Loss: 0.3835\nStep 120/158 | Loss: 0.4377\nStep 130/158 | Loss: 0.5086\nStep 140/158 | Loss: 0.3490\nStep 150/158 | Loss: 0.4017\nEpoch: 06\n\tTrain Loss: 0.404\n\t Val. Loss: 0.501 |  Val. F1: 84.78%\nStep 10/158 | Loss: 0.3651\nStep 20/158 | Loss: 0.2892\nStep 30/158 | Loss: 0.3090\nStep 40/158 | Loss: 0.2872\nStep 50/158 | Loss: 0.3130\nStep 60/158 | Loss: 0.3707\nStep 70/158 | Loss: 0.4284\nStep 80/158 | Loss: 0.3643\nStep 90/158 | Loss: 0.3558\nStep 100/158 | Loss: 0.4361\nStep 110/158 | Loss: 0.3677\nStep 120/158 | Loss: 0.3368\nStep 130/158 | Loss: 0.3109\nStep 140/158 | Loss: 0.4427\nStep 150/158 | Loss: 0.3595\nEpoch: 07\n\tTrain Loss: 0.353\n\t Val. Loss: 0.486 |  Val. F1: 85.11%\nStep 10/158 | Loss: 0.2623\nStep 20/158 | Loss: 0.2783\nStep 30/158 | Loss: 0.3352\nStep 40/158 | Loss: 0.4965\nStep 50/158 | Loss: 0.3288\nStep 60/158 | Loss: 0.2655\nStep 70/158 | Loss: 0.2763\nStep 80/158 | Loss: 0.3483\nStep 90/158 | Loss: 0.2602\nStep 100/158 | Loss: 0.3248\nStep 110/158 | Loss: 0.4899\nStep 120/158 | Loss: 0.3299\nStep 130/158 | Loss: 0.3277\nStep 140/158 | Loss: 0.3397\nStep 150/158 | Loss: 0.3442\nEpoch: 08\n\tTrain Loss: 0.336\n\t Val. Loss: 0.469 |  Val. F1: 85.55%\nStep 10/158 | Loss: 0.4145\nStep 20/158 | Loss: 0.3066\nStep 30/158 | Loss: 0.3142\nStep 40/158 | Loss: 0.3998\nStep 50/158 | Loss: 0.3095\nStep 60/158 | Loss: 0.4084\nStep 70/158 | Loss: 0.2942\nStep 80/158 | Loss: 0.2520\nStep 90/158 | Loss: 0.3937\nStep 100/158 | Loss: 0.2732\nStep 110/158 | Loss: 0.2414\nStep 120/158 | Loss: 0.2663\nStep 130/158 | Loss: 0.3427\nStep 140/158 | Loss: 0.4072\nStep 150/158 | Loss: 0.3508\nEpoch: 09\n\tTrain Loss: 0.319\n\t Val. Loss: 0.462 |  Val. F1: 85.54%\nStep 10/158 | Loss: 0.4079\nStep 20/158 | Loss: 0.2932\nStep 30/158 | Loss: 0.3682\nStep 40/158 | Loss: 0.2026\nStep 50/158 | Loss: 0.3582\nStep 60/158 | Loss: 0.2596\nStep 70/158 | Loss: 0.2628\nStep 80/158 | Loss: 0.3818\nStep 90/158 | Loss: 0.3503\nStep 100/158 | Loss: 0.2333\nStep 110/158 | Loss: 0.2224\nStep 120/158 | Loss: 0.2913\nStep 130/158 | Loss: 0.2927\nStep 140/158 | Loss: 0.4714\nStep 150/158 | Loss: 0.3241\nEpoch: 10\n\tTrain Loss: 0.306\n\t Val. Loss: 0.441 |  Val. F1: 85.79%\n\nĐang đánh giá trên tập Test...\nTest Loss: 0.450 | Test F1: 85.52%\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# ","metadata":{}}]}